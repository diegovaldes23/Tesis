{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15323c0e-ae3e-4c85-92c3-a95b7a5c6726",
   "metadata": {},
   "source": [
    "### Extracción de características a nivel de clip (UCF-Crime)\n",
    "\n",
    "Este notebook utiliza como entrada el archivo `processed/index_clips.csv`\n",
    "\n",
    "El archivo `index_clips.csv` contiene, para cada clip temporal:\n",
    "- La ruta al video original.\n",
    "- El rango temporal del clip (`start_frame`, `end_frame`).\n",
    "- La partición correspondiente (`train`, `val`, `test`).\n",
    "- La etiqueta binaria (normal vs anómalo) y la categoría asociada.\n",
    "- Los parámetros de segmentación utilizados (longitud del clip y solapamiento).\n",
    "\n",
    "A partir de este índice, se cargan los frames correspondientes a cada clip y se transforman en la\n",
    "representación requerida por los modelos evaluados, sin redefinir ni modificar la composición del\n",
    "conjunto experimental.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95097f3b-37a1-459c-a65d-f2e745d75125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo cargado correctamente\n",
      "Número total de clips: 145356\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>y</th>\n",
       "      <th>category</th>\n",
       "      <th>path</th>\n",
       "      <th>clip_idx</th>\n",
       "      <th>start_frame</th>\n",
       "      <th>end_frame</th>\n",
       "      <th>clip_len</th>\n",
       "      <th>stride</th>\n",
       "      <th>fps</th>\n",
       "      <th>n_frames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>/home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>/home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>48</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>/home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>/home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>80</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>/home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>96</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split  y category                                               path  \\\n",
       "0  train  0   Normal  /home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...   \n",
       "1  train  0   Normal  /home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...   \n",
       "2  train  0   Normal  /home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...   \n",
       "3  train  0   Normal  /home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...   \n",
       "4  train  0   Normal  /home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...   \n",
       "\n",
       "   clip_idx  start_frame  end_frame  clip_len  stride   fps  n_frames  \n",
       "0         0            0         32        32      16  30.0      2016  \n",
       "1         1           16         48        32      16  30.0      2016  \n",
       "2         2           32         64        32      16  30.0      2016  \n",
       "3         3           48         80        32      16  30.0      2016  \n",
       "4         4           64         96        32      16  30.0      2016  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Ruta al índice de clips\n",
    "INDEX_CLIPS_PATH = Path(\"processed/index_clips.csv\")\n",
    "\n",
    "# Verificar existencia\n",
    "assert INDEX_CLIPS_PATH.exists(), f\"No se encuentra el archivo: {INDEX_CLIPS_PATH}\"\n",
    "\n",
    "# Cargar CSV\n",
    "df_clips = pd.read_csv(INDEX_CLIPS_PATH)\n",
    "\n",
    "print(\"Archivo cargado correctamente\")\n",
    "print(\"Número total de clips:\", len(df_clips))\n",
    "\n",
    "display(df_clips.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d46e76-697f-4df4-9bda-e87b15ccb54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clips por split:\n",
      "split\n",
      "train    106527\n",
      "val       19793\n",
      "test      19036\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Clips por clase (y):\n",
      "y\n",
      "0    73870\n",
      "1    71486\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Clips con rangos inválidos: 0\n",
      "Clips con path inexistente: 0\n"
     ]
    }
   ],
   "source": [
    "# Distribución por split\n",
    "print(\"Clips por split:\")\n",
    "print(df_clips[\"split\"].value_counts())\n",
    "\n",
    "# Distribución por clase\n",
    "print(\"\\nClips por clase (y):\")\n",
    "print(df_clips[\"y\"].value_counts())\n",
    "\n",
    "# Verificar rangos temporales válidos\n",
    "invalid_ranges = df_clips[df_clips[\"end_frame\"] <= df_clips[\"start_frame\"]]\n",
    "print(\"\\nClips con rangos inválidos:\", len(invalid_ranges))\n",
    "\n",
    "# Verificar paths únicos y existencia\n",
    "missing_paths = df_clips[~df_clips[\"path\"].apply(lambda p: Path(p).exists())]\n",
    "print(\"Clips con path inexistente:\", len(missing_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0bb3684-eb80-4a09-af81-8f7da8f2cb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Videos con clips en más de un split: 0\n"
     ]
    }
   ],
   "source": [
    "# Verificar que un mismo video no aparezca en más de un split\n",
    "video_split_counts = df_clips.groupby(\"path\")[\"split\"].nunique()\n",
    "n_leak = int((video_split_counts > 1).sum())\n",
    "\n",
    "print(\"Videos con clips en más de un split:\", n_leak)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939a4f44-3d6f-4c7e-a80c-9295a8a75bad",
   "metadata": {},
   "source": [
    "# Carga y congelamiento del encoder VideoCLIP (X-CLIP)\n",
    "\n",
    "Objetivo:\n",
    "Cargar un modelo preentrenado tipo VideoCLIP (X-CLIP), moverlo a GPU/CPU según disponibilidad y congelar sus parámetros para utilizarlo como feature extractor.\n",
    "\n",
    "Entradas:\n",
    "\n",
    "VIDEOCLIP_CKPT: string con el identificador del checkpoint (p. ej., \"microsoft/xclip-base-patch32\").\n",
    "\n",
    "DEVICE: dispositivo de cómputo seleccionado automáticamente (\"cuda\" o \"cpu\").\n",
    "\n",
    "Salidas:\n",
    "\n",
    "processor: XCLIPProcessor con configuración de preprocesamiento (incluye image_mean y image_std).\n",
    "\n",
    "encoder: XCLIPModel preentrenado en modo evaluación y con parámetros congelados (requires_grad=False).\n",
    "\n",
    "Impresiones en consola: dispositivo seleccionado, checkpoint cargado y valores mean/std para normalización.\n",
    "\n",
    "Descripción técnica:\n",
    "Se inicializa el procesador asociado al checkpoint y se carga el modelo X-CLIP. Luego se activa modo eval() para deshabilitar capas dependientes de entrenamiento (p. ej., dropout) y se congelan los parámetros para evitar actualización durante el entrenamiento del clasificador posterior (MLP).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04839135-b6d5-4517-a251-cd81de2e4478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DIINF/dvaldes/venvs/tesis/lib/python3.10/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK loaded: microsoft/xclip-base-patch32\n",
      "processor mean/std: [0.485, 0.456, 0.406] [0.229, 0.224, 0.225]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import XCLIPModel, XCLIPProcessor\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "VIDEOCLIP_CKPT = \"microsoft/xclip-base-patch32\"\n",
    "\n",
    "processor = XCLIPProcessor.from_pretrained(VIDEOCLIP_CKPT)\n",
    "\n",
    "encoder = XCLIPModel.from_pretrained(VIDEOCLIP_CKPT).to(DEVICE)\n",
    "encoder.eval()\n",
    "\n",
    "for p in encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"OK loaded:\", VIDEOCLIP_CKPT)\n",
    "print(\n",
    "    \"processor mean/std:\",\n",
    "    processor.image_processor.image_mean,\n",
    "    processor.image_processor.image_std\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebfdfea-61ff-4d3b-8a6b-c183e83e3759",
   "metadata": {},
   "source": [
    "Definición del input y Dataset de clips (muestreo temporal + preprocesamiento)\n",
    "\n",
    "Objetivo:\n",
    "Construir un Dataset que, a partir de un índice de clips (rango de frames por video), cargue T frames uniformemente muestreados, los preprocese (RGB, resize, normalización) y devuelva un tensor listo para el encoder.\n",
    "\n",
    "Entradas:\n",
    "\n",
    "T: número de frames por clip (p. ej., 8).\n",
    "\n",
    "IMG_SIZE: tamaño espacial objetivo (p. ej., 224×224).\n",
    "\n",
    "df: DataFrame con al menos: path, start_frame, end_frame, y.\n",
    "\n",
    "processor.image_processor.image_mean/std: estadísticas de normalización del checkpoint X-CLIP.\n",
    "\n",
    "Salidas:\n",
    "\n",
    "clip: tensor torch.float32 con forma (C, T, H, W) = (3, T, 224, 224).\n",
    "\n",
    "y: etiqueta torch.long (0/1).\n",
    "\n",
    "Descripción técnica:\n",
    "Para cada fila del índice, se abre el video con OpenCV y se seleccionan T frames distribuidos uniformemente entre start_frame y end_frame. Cada frame se convierte a RGB, se redimensiona a IMG_SIZE y se normaliza usando mean/std del modelo (CLIP-style). El clip resultante se organiza en el formato (C, T, H, W), que luego se reordena a (B, T, C, H, W) antes de ingresar al encoder.\n",
    "\n",
    "Nota de implementación (reproducibilidad):\n",
    "Este bloque asume que la variable global processor ya fue creada previamente. Para mayor robustez, se recomienda pasar mean/std como parámetros al Dataset y evitar dependencias globales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0595dee-afe4-4a5b-b866-c9004fc6d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parámetros del input\n",
    "T = 8                \n",
    "IMG_SIZE = 224       \n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "def uniform_sample_indices(start_f: int, end_f: int, T: int):\n",
    "    n = max(1, end_f - start_f)\n",
    "    idx = np.linspace(0, n - 1, T).round().astype(int)\n",
    "    return (start_f + idx).astype(int)\n",
    "\n",
    "class ClipDataset(Dataset):\n",
    "    def __init__(self, df, T=8, img_size=224):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.T = T\n",
    "        self.img_size = img_size\n",
    "        self.mean = np.array(processor.image_processor.image_mean, dtype=np.float32)\n",
    "        self.std  = np.array(processor.image_processor.image_std, dtype=np.float32)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        path = row[\"path\"]\n",
    "        start_f = int(row[\"start_frame\"])\n",
    "        end_f   = int(row[\"end_frame\"])\n",
    "        y = int(row[\"y\"])\n",
    "\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        if not cap.isOpened():\n",
    "            raise RuntimeError(f\"No pude abrir video: {path}\")\n",
    "\n",
    "        frame_ids = uniform_sample_indices(start_f, end_f, self.T)\n",
    "\n",
    "        frames = []\n",
    "        last_good = None\n",
    "        for fid in frame_ids:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(fid))\n",
    "            ok, frame = cap.read()\n",
    "\n",
    "            if not ok:\n",
    "                if last_good is None:\n",
    "                    frame = np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8)\n",
    "                else:\n",
    "                    frame = last_good\n",
    "            else:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = cv2.resize(frame, (self.img_size, self.img_size), interpolation=cv2.INTER_LINEAR)\n",
    "                last_good = frame\n",
    "\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # (T,H,W,C) -> float [0,1]\n",
    "        arr = np.stack(frames).astype(np.float32) / 255.0\n",
    "        arr = (arr - self.mean) / self.std\n",
    "\n",
    "        # -> (C,T,H,W)\n",
    "        arr = np.transpose(arr, (3, 0, 1, 2))\n",
    "        clip = torch.from_numpy(arr)  # float32\n",
    "\n",
    "        return clip, torch.tensor(y, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd1539bd-8a5b-4a80-a6d5-1c0345e1c20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb shape: torch.Size([16, 3, 8, 224, 224])\n",
      "yb shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "df_train = df_clips[df_clips[\"split\"]==\"train\"].copy()\n",
    "df_val   = df_clips[df_clips[\"split\"]==\"val\"].copy()\n",
    "df_test  = df_clips[df_clips[\"split\"]==\"test\"].copy()\n",
    "\n",
    "train_ds = ClipDataset(df_train, T=T, img_size=IMG_SIZE)\n",
    "val_ds   = ClipDataset(df_val,   T=T, img_size=IMG_SIZE)\n",
    "test_ds  = ClipDataset(df_test,  T=T, img_size=IMG_SIZE)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"xb shape:\", xb.shape)  # (B, 3, T, 224, 224)\n",
    "print(\"yb shape:\", yb.shape)  # (B,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0d1f31-ebe1-4cd6-8099-a5e93d024555",
   "metadata": {},
   "source": [
    "Extracción de embeddings de video (modo unimodal con X-CLIP)\n",
    "\n",
    "Objetivo:\n",
    "Obtener un embedding vectorial fijo por clip de video utilizando únicamente la rama visual del modelo X-CLIP.\n",
    "\n",
    "Entradas:\n",
    "\n",
    "encoder: modelo XCLIPModel preentrenado y congelado.\n",
    "\n",
    "pixel_values: tensor con forma (B, T, C, H, W), donde:\n",
    "\n",
    "B = batch size\n",
    "\n",
    "T = número de frames\n",
    "\n",
    "C = canales (3)\n",
    "\n",
    "H, W = dimensiones espaciales (224×224)\n",
    "\n",
    "Salidas:\n",
    "\n",
    "video_embeds: tensor con forma (B, D), donde D = 512 (dimensión del embedding del modelo base).\n",
    "\n",
    "Descripción técnica:\n",
    "Cada frame del clip se procesa individualmente mediante el encoder visual (vision_model).\n",
    "Las representaciones resultantes se proyectan al espacio CLIP (visual_projection) para obtener embeddings por frame.\n",
    "Posteriormente, estos embeddings se reorganizan en secuencia temporal (B, T, D) y se integran mediante el módulo temporal MIT del modelo, produciendo un embedding global por clip.\n",
    "\n",
    "El bloque final de uso:\n",
    "\n",
    "Reordena el tensor de entrada de (B, C, T, H, W) a (B, T, C, H, W).\n",
    "\n",
    "Ejecuta la extracción bajo torch.no_grad() para evitar cálculo de gradientes.\n",
    "\n",
    "Verifica dimensionalidades esperadas.\n",
    "\n",
    "Este embedding es el que posteriormente alimenta el clasificador MLP para la tarea de detección binaria (normal/anómalo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecba8164-bb31-423f-8e7c-ee28c76b0640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values shape: torch.Size([16, 8, 3, 224, 224])\n",
      "video_embeds shape: torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def xclip_video_embeds_unimodal(encoder, pixel_values: torch.Tensor):\n",
    "    \"\"\"\n",
    "    pixel_values: (B, T, C, H, W)\n",
    "    return: (B, D) video embedding\n",
    "    \"\"\"\n",
    "    B, T, C, H, W = pixel_values.shape\n",
    "\n",
    "    # 1) Aplanar frames: (B*T, C, H, W)\n",
    "    flat = pixel_values.reshape(B * T, C, H, W).contiguous()\n",
    "\n",
    "    # 2) Vision encoder por frame\n",
    "    vision_out = encoder.vision_model(pixel_values=flat)\n",
    "\n",
    "    if isinstance(vision_out, (tuple, list)):\n",
    "        frame_pooled = vision_out[1]\n",
    "    else:\n",
    "        frame_pooled = vision_out.pooler_output\n",
    "\n",
    "    # 3) Proyección (emb por frame)\n",
    "    frame_embeds = encoder.visual_projection(frame_pooled)  # (B*T, D)\n",
    "\n",
    "    # 4) Secuencia temporal: (B, T, D)\n",
    "    cls_features = frame_embeds.view(B, T, -1)\n",
    "\n",
    "    # 5) Integración temporal (MIT)\n",
    "    mit_out = encoder.mit(cls_features)\n",
    "\n",
    "    if isinstance(mit_out, (tuple, list)):\n",
    "        video_embeds = mit_out[1] if len(mit_out) > 1 else mit_out[0]\n",
    "    else:\n",
    "        video_embeds = mit_out.pooler_output\n",
    "\n",
    "    return video_embeds\n",
    "\n",
    "\n",
    "# ====== USO ======\n",
    "xb = xb.to(DEVICE)\n",
    "pixel_values = xb.permute(0, 2, 1, 3, 4).contiguous()  # (B, T, C, H, W)\n",
    "\n",
    "with torch.no_grad():\n",
    "    video_embeds = xclip_video_embeds_unimodal(encoder, pixel_values)\n",
    "\n",
    "print(\"pixel_values shape:\", pixel_values.shape)\n",
    "print(\"video_embeds shape:\", video_embeds.shape)  # (B, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42a9b83e-93ac-40f7-85fb-112dc544ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_embeds shape: torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    video_embeds = xclip_video_embeds_unimodal(encoder, pixel_values)\n",
    "\n",
    "print(\"video_embeds shape:\", video_embeds.shape)  # (B, 512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bfd13c-cd67-4c65-b721-873cb2dfc798",
   "metadata": {},
   "source": [
    "# 3 Extracción de emebdings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a640ae4-a3d1-4fff-ae90-91bb8381395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def create_memmap(path, shape, dtype=\"float16\"):\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    return np.memmap(path, mode=\"w+\", dtype=dtype, shape=shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a026e101-f4db-498f-91a3-9a30383df5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo con un batch dummy ya calculado\n",
    "D = int(video_embeds.shape[1])   # debería ser 512\n",
    "print(\"Embedding dim:\", D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b299f907-ab7b-416e-9ef6-9bc339d3ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_clips[df_clips[\"split\"]==\"train\"].copy()\n",
    "df_val   = df_clips[df_clips[\"split\"]==\"val\"].copy()\n",
    "df_test  = df_clips[df_clips[\"split\"]==\"test\"].copy()\n",
    "\n",
    "train_ds = ClipDataset(df_train, T=T, img_size=IMG_SIZE)\n",
    "val_ds   = ClipDataset(df_val,   T=T, img_size=IMG_SIZE)\n",
    "test_ds  = ClipDataset(df_test,  T=T, img_size=IMG_SIZE)\n",
    "\n",
    "print(len(train_ds), len(val_ds), len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9789118e-c8d4-4af3-ab81-43184a9fe3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train\n",
    "X_train = create_memmap(\"processed/emb_xclip_video_train.mmap\", (len(train_ds), D), dtype=\"float16\")\n",
    "y_train = create_memmap(\"processed/y_train.mmap\", (len(train_ds),), dtype=\"int8\")\n",
    "\n",
    "# Val\n",
    "X_val = create_memmap(\"processed/emb_xclip_video_val.mmap\", (len(val_ds), D), dtype=\"float16\")\n",
    "y_val = create_memmap(\"processed/y_val.mmap\", (len(val_ds),), dtype=\"int8\")\n",
    "\n",
    "# Test\n",
    "X_test = create_memmap(\"processed/emb_xclip_video_test.mmap\", (len(test_ds), D), dtype=\"float16\")\n",
    "y_test = create_memmap(\"processed/y_test.mmap\", (len(test_ds),), dtype=\"int8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a66f1-ac93-411d-b477-e1b6d73e6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def create_memmap(path, shape, dtype=\"float16\"):\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    return np.memmap(path, mode=\"w+\", dtype=dtype, shape=shape)\n",
    "\n",
    "def extract_embeddings_xclip(loader, encoder, X_mm, y_mm, split_name=\"train\"):\n",
    "    encoder.eval()\n",
    "\n",
    "    total_batches = len(loader)\n",
    "    print(f\"\\nExtracting {split_name.upper()} embeddings (X-CLIP)...\")\n",
    "    print(f\"Total batches: {total_batches}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        offset = 0\n",
    "\n",
    "        for xb, yb in tqdm(loader, desc=f\"{split_name}\", leave=True):\n",
    "            xb = xb.to(DEVICE)  # (B, C, T, H, W)\n",
    "            pixel_values = xb.permute(0, 2, 1, 3, 4).contiguous()  # (B, T, C, H, W)\n",
    "\n",
    "            # ✅ embedding (B, 512)\n",
    "            video_embeds = xclip_video_embeds_unimodal(encoder, pixel_values)\n",
    "\n",
    "            emb_np = video_embeds.detach().cpu().numpy().astype(X_mm.dtype, copy=False)\n",
    "            y_np = yb.numpy().astype(y_mm.dtype, copy=False)\n",
    "\n",
    "            bs = emb_np.shape[0]\n",
    "            X_mm[offset:offset+bs] = emb_np\n",
    "            y_mm[offset:offset+bs] = y_np\n",
    "            offset += bs\n",
    "\n",
    "    X_mm.flush()\n",
    "    y_mm.flush()\n",
    "    print(f\"{split_name.upper()} extraction done.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4671d21c-d1a8-4d72-991a-7d28e922b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_embeddings_xclip(train_loader, encoder, X_train, y_train, split_name=\"train\")\n",
    "extract_embeddings_xclip(val_loader,   encoder, X_val,   y_val,   split_name=\"val\")\n",
    "extract_embeddings_xclip(test_loader,  encoder, X_test,  y_test,  split_name=\"test\")\n",
    "\n",
    "print(\"\\nAll embeddings extracted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3273c9a-ba24-4671-af74-ee70558b310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Val:  \", X_val.shape, y_val.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aab756-590c-4e40-8192-cc6e39cc3a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sanity_mm_fp32(X_mm, y_mm, name=\"split\", n=5000):\n",
    "    n = min(n, len(y_mm))\n",
    "    X = np.array(X_mm[:n], dtype=np.float32)   # <- clave\n",
    "    y = np.array(y_mm[:n], dtype=np.int64)\n",
    "\n",
    "    print(f\"\\n[{name}]\")\n",
    "    print(\"  finite:\", np.isfinite(X).all())\n",
    "    print(\"  mean:\", float(X.mean()))\n",
    "    print(\"  std:\",  float(X.std()))\n",
    "    print(\"  min/max:\", float(X.min()), float(X.max()))\n",
    "    print(\"  y counts:\", {int(v): int((y==v).sum()) for v in np.unique(y)})\n",
    "\n",
    "sanity_mm_fp32(X_train, y_train, \"train\")\n",
    "sanity_mm_fp32(X_val,   y_val,   \"val\")\n",
    "sanity_mm_fp32(X_test,  y_test,  \"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7275a1-987b-4f03-9aa9-1ef1448e6c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_train[:5000], dtype=np.float32)\n",
    "print(\"Any inf:\", np.isinf(X).any())\n",
    "print(\"Any nan:\", np.isnan(X).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40211f38-faf1-42e2-81f5-81b37583f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# usa una muestra pequeña para que sea rápido\n",
    "ntr = min(20000, len(y_train))\n",
    "nva = min(8000,  len(y_val))\n",
    "\n",
    "Xtr = np.array(X_train[:ntr], dtype=np.float32)\n",
    "ytr = np.array(y_train[:ntr], dtype=np.int64)\n",
    "\n",
    "Xva = np.array(X_val[:nva], dtype=np.float32)\n",
    "yva = np.array(y_val[:nva], dtype=np.int64)\n",
    "\n",
    "clf = LogisticRegression(max_iter=200, n_jobs=-1)\n",
    "clf.fit(Xtr, ytr)\n",
    "\n",
    "pva = clf.predict_proba(Xva)[:, 1]\n",
    "auc = roc_auc_score(yva, pva)\n",
    "print(\"Sanity AUC (LogReg):\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f3f255-cfb4-4b0f-b7fb-693c129a2407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "manifest = {\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"model\": VIDEOCLIP_CKPT,  # <-- cambia esto\n",
    "    \"T\": T,\n",
    "    \"img_size\": IMG_SIZE,\n",
    "    \"embedding_dim\": int(X_train.shape[1]),  # debería ser 512\n",
    "    \"files\": {\n",
    "        \"X_train\": \"processed/emb_xclip_video_train.mmap\",\n",
    "        \"y_train\": \"processed/y_train.mmap\",\n",
    "        \"X_val\":   \"processed/emb_xclip_video_val.mmap\",\n",
    "        \"y_val\":   \"processed/y_val.mmap\",\n",
    "        \"X_test\":  \"processed/emb_xclip_video_test.mmap\",\n",
    "        \"y_test\":  \"processed/y_test.mmap\",\n",
    "    }\n",
    "}\n",
    "\n",
    "Path(\"processed\").mkdir(exist_ok=True)\n",
    "\n",
    "with open(\"processed/manifest_xclip_video.json\", \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", \"processed/manifest_xclip_video.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tesis)",
   "language": "python",
   "name": "tesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
