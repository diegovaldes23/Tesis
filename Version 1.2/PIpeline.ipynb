{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3a5f619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset root: /home/diego/Escritorio/Pruebas/tesispython/UCF_Crime False\n",
      "Processed dir: /home/diego/Escritorio/Pruebas/tesispython/UCF_Crime/processed False\n",
      "Index exists: False\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "NOTEBOOK_DIR = Path(\"\")\n",
    "\n",
    "DATASET_ROOT = NOTEBOOK_DIR / \"UCF_Crime\"\n",
    "PROCESSED_DIR = DATASET_ROOT / \"processed\"\n",
    "\n",
    "print(\"Dataset root:\", DATASET_ROOT, DATASET_ROOT.exists())\n",
    "print(\"Processed dir:\", PROCESSED_DIR, PROCESSED_DIR.exists())\n",
    "print(\"Index exists:\", (PROCESSED_DIR / \"preprocess_index.csv\").exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f21919",
   "metadata": {},
   "source": [
    "# Paso 1 — Obtención y validación de datos (UCF-Crime)\n",
    "\n",
    "## Objetivo\n",
    "Validar que el dataset UCF-Crime está correctamente ubicado y estructurado en disco, y que las carpetas/clases contienen videos legibles antes de ejecutar cualquier preprocesamiento, extracción de embeddings o entrenamiento.\n",
    "\n",
    "## Estructura esperada\n",
    "La estructura de referencia del dataset (según README oficial) se organiza como:\n",
    "\n",
    "`UCF_CRIME/Videos/<Clase>/video.ext`\n",
    "\n",
    "En esta etapa se trabaja con:\n",
    "- **Fase 1 (Validación técnica):** subconjunto de anomalías (Abuse, Arrest, Arson, Assault) + normal provisional.\n",
    "- **Fase 2 (Experimento completo):** anomalías completas + normales oficiales (`Training_Normal_Videos_Anomaly` y `Testing_Normal_Videos_Anomaly`) + splits oficiales.\n",
    "\n",
    "## Resultado esperado\n",
    "El notebook debe imprimir mensajes del tipo:\n",
    "- `Abuse: cargado correctamente (N videos)`\n",
    "- `Arrest: cargado correctamente (N videos)`\n",
    "y generar un reporte reproducible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e76e7de",
   "metadata": {},
   "source": [
    "## Paso 1.1 — Configuración (Código)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62d12e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_ROOT: UCF_Crime\n",
      "VIDEOS_DIR: UCF_Crime/Videos\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# =========================\n",
    "# CONFIGURACIÓN PRINCIPAL\n",
    "# =========================\n",
    "DATASET_ROOT = Path(\"UCF_Crime\")\n",
    "VIDEOS_DIR = DATASET_ROOT / \"Videos\"\n",
    "\n",
    "# Extensiones soportadas\n",
    "VIDEO_EXTS = {\".mp4\", \".avi\", \".mkv\", \".mov\", \".webm\"}\n",
    "\n",
    "# Clases oficiales UCF-Crime (13 anomalías)\n",
    "ANOMALY_FOLDERS = [\n",
    "    \"Abuse\", \"Arrest\", \"Arson\", \"Assault\", \"Burglary\", \"Explosion\",\n",
    "    \"Fighting\", \"Robbery\", \"RoadAccidents\", \"Shooting\", \"Shoplifting\",\n",
    "    \"Stealing\", \"Vandalism\"\n",
    "]\n",
    "\n",
    "# Normales oficiales (anomaly detection)\n",
    "NORMAL_OFFICIAL = [\"Training_Normal_Videos_Anomaly\", \"Testing_Normal_Videos_Anomaly\"]\n",
    "\n",
    "# Normal opcional (event recognition) — puede usarse como placeholder en Fase 1\n",
    "NORMAL_OPTIONAL = [\"Normal_Videos_event\"]\n",
    "\n",
    "print(\"DATASET_ROOT:\", DATASET_ROOT)\n",
    "print(\"VIDEOS_DIR:\", VIDEOS_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4b3266",
   "metadata": {},
   "source": [
    "## Paso 1.2 — Funciones de validación (Código)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06c219f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_videos(folder: Path):\n",
    "    \"\"\"Lista videos válidos dentro de una carpeta (recursivo).\"\"\"\n",
    "    if not folder.exists() or not folder.is_dir():\n",
    "        return []\n",
    "    vids = []\n",
    "    for p in folder.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in VIDEO_EXTS:\n",
    "            vids.append(p)\n",
    "    return sorted(vids)\n",
    "\n",
    "def check_folder(name: str, folder: Path, required: bool):\n",
    "    \"\"\"\n",
    "    Revisa existencia + conteo de videos.\n",
    "    Retorna dict con estado para reporte.\n",
    "    \"\"\"\n",
    "    info = {\n",
    "        \"name\": name,\n",
    "        \"path\": str(folder),\n",
    "        \"exists\": folder.exists(),\n",
    "        \"video_count\": 0,\n",
    "        \"status\": None\n",
    "    }\n",
    "\n",
    "    if not folder.exists():\n",
    "        info[\"status\"] = \"MISSING\" if required else \"NOT_PRESENT_OPTIONAL\"\n",
    "        return info\n",
    "\n",
    "    vids = list_videos(folder)\n",
    "    info[\"video_count\"] = len(vids)\n",
    "    if len(vids) == 0:\n",
    "        info[\"status\"] = \"EMPTY\"\n",
    "    else:\n",
    "        info[\"status\"] = \"OK\"\n",
    "\n",
    "    return info\n",
    "\n",
    "def print_status(info):\n",
    "    \"\"\"Imprime estado en formato 'presentable'.\"\"\"\n",
    "    if info[\"status\"] == \"OK\":\n",
    "        print(f\"{info['name']}: cargado correctamente ({info['video_count']} videos)\")\n",
    "    elif info[\"status\"] == \"EMPTY\":\n",
    "        print(f\"{info['name']}: VACÍA (0 videos)\")\n",
    "    elif info[\"status\"] == \"MISSING\":\n",
    "        print(f\"{info['name']}: FALTA (no existe carpeta)\")\n",
    "    else:\n",
    "        print(f\"{info['name']}: NO PRESENTE (opcional)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1631392",
   "metadata": {},
   "source": [
    "## Paso 1.3 — Validación de estructura base (Código)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75f7342d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Paso 1.3 — Validación de estructura base ===\n",
      "OK: carpeta 'Videos' encontrada en: UCF_Crime/Videos\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Paso 1.3 — Validación de estructura base ===\")\n",
    "\n",
    "if not VIDEOS_DIR.exists():\n",
    "    raise FileNotFoundError(f\"No existe la carpeta esperada: {VIDEOS_DIR}\")\n",
    "\n",
    "print(f\"OK: carpeta 'Videos' encontrada en: {VIDEOS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c0ff5e",
   "metadata": {},
   "source": [
    "## Paso 1.4 — Validación de clases (subconjunto y detección global) (Código)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8eb21b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Paso 1.4 — Validación de clases disponibles ===\n",
      "\n",
      "---- Anomalías (oficiales) ----\n",
      "Abuse: cargado correctamente (50 videos)\n",
      "Arrest: cargado correctamente (50 videos)\n",
      "Arson: cargado correctamente (50 videos)\n",
      "Assault: cargado correctamente (50 videos)\n",
      "Burglary: NO PRESENTE (opcional)\n",
      "Explosion: NO PRESENTE (opcional)\n",
      "Fighting: NO PRESENTE (opcional)\n",
      "Robbery: NO PRESENTE (opcional)\n",
      "RoadAccidents: NO PRESENTE (opcional)\n",
      "Shooting: NO PRESENTE (opcional)\n",
      "Shoplifting: NO PRESENTE (opcional)\n",
      "Stealing: NO PRESENTE (opcional)\n",
      "Vandalism: NO PRESENTE (opcional)\n",
      "\n",
      "---- Normales (oficiales anomaly detection) ----\n",
      "Training_Normal_Videos_Anomaly: NO PRESENTE (opcional)\n",
      "Testing_Normal_Videos_Anomaly: NO PRESENTE (opcional)\n",
      "\n",
      "---- Normales (opcionales / placeholder) ----\n",
      "Normal_Videos_event: cargado correctamente (50 videos)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Paso 1.4 — Validación de clases disponibles ===\")\n",
    "\n",
    "report = {\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"dataset_root\": str(DATASET_ROOT),\n",
    "    \"videos_dir\": str(VIDEOS_DIR),\n",
    "    \"anomalies\": [],\n",
    "    \"normals_official\": [],\n",
    "    \"normals_optional\": [],\n",
    "}\n",
    "\n",
    "# 1) Anomalías (listadas como opcionales porque puede faltar parte del dataset aún)\n",
    "print(\"\\n---- Anomalías (oficiales) ----\")\n",
    "for cls in ANOMALY_FOLDERS:\n",
    "    info = check_folder(cls, VIDEOS_DIR / cls, required=False)\n",
    "    report[\"anomalies\"].append(info)\n",
    "    print_status(info)\n",
    "\n",
    "# 2) Normales oficiales\n",
    "print(\"\\n---- Normales (oficiales anomaly detection) ----\")\n",
    "for nf in NORMAL_OFFICIAL:\n",
    "    info = check_folder(nf, VIDEOS_DIR / nf, required=False)\n",
    "    report[\"normals_official\"].append(info)\n",
    "    print_status(info)\n",
    "\n",
    "# 3) Normal opcional (placeholder)\n",
    "print(\"\\n---- Normales (opcionales / placeholder) ----\")\n",
    "for nf in NORMAL_OPTIONAL:\n",
    "    info = check_folder(nf, VIDEOS_DIR / nf, required=False)\n",
    "    report[\"normals_optional\"].append(info)\n",
    "    print_status(info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b24744a",
   "metadata": {},
   "source": [
    "## Paso 1.5 — Checklist Fase 1 vs Fase 2 (Código)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98e21fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Paso 1.5 — Checklist de preparación ===\n",
      "Anomalías presentes: 4 / 13 (videos: 200)\n",
      "Normales oficiales presentes: 0 / 2 (videos: 0)\n",
      "Normales placeholder presentes: 1 / 1 (videos: 50)\n",
      "\n",
      "Resultado:\n",
      "FASE 1 (validación pipeline): OK\n",
      "FASE 2 (experimento completo): PENDIENTE (faltan normales oficiales)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Paso 1.5 — Checklist de preparación ===\")\n",
    "\n",
    "# Conteos\n",
    "anomaly_ok = [x for x in report[\"anomalies\"] if x[\"status\"] == \"OK\"]\n",
    "normal_off_ok = [x for x in report[\"normals_official\"] if x[\"status\"] == \"OK\"]\n",
    "normal_opt_ok = [x for x in report[\"normals_optional\"] if x[\"status\"] == \"OK\"]\n",
    "\n",
    "total_anomaly_videos = sum(x[\"video_count\"] for x in anomaly_ok)\n",
    "total_normal_off = sum(x[\"video_count\"] for x in normal_off_ok)\n",
    "total_normal_opt = sum(x[\"video_count\"] for x in normal_opt_ok)\n",
    "\n",
    "print(f\"Anomalías presentes: {len(anomaly_ok)} / {len(ANOMALY_FOLDERS)} (videos: {total_anomaly_videos})\")\n",
    "print(f\"Normales oficiales presentes: {len(normal_off_ok)} / 2 (videos: {total_normal_off})\")\n",
    "print(f\"Normales placeholder presentes: {len(normal_opt_ok)} / 1 (videos: {total_normal_opt})\")\n",
    "\n",
    "# Fase 1: al menos 1 anomalía + algún normal\n",
    "fase1_ok = (len(anomaly_ok) > 0) and ((total_normal_off + total_normal_opt) > 0)\n",
    "\n",
    "# Fase 2: normales oficiales ambos presentes\n",
    "fase2_ok = (len(normal_off_ok) == 2)\n",
    "\n",
    "print(\"\\nResultado:\")\n",
    "print(\"FASE 1 (validación pipeline):\", \"OK\" if fase1_ok else \"NO OK\")\n",
    "print(\"FASE 2 (experimento completo):\", \"OK\" if fase2_ok else \"PENDIENTE (faltan normales oficiales)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1315ab49",
   "metadata": {},
   "source": [
    "## Paso 1.6 — Validación explícita de tu subconjunto + normal elegido (Código)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b83c45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Paso 1.6 — Subconjunto de trabajo (Fase 1) ===\n",
      "Abuse: cargado correctamente (50 videos)\n",
      "Arrest: cargado correctamente (50 videos)\n",
      "Arson: cargado correctamente (50 videos)\n",
      "Assault: cargado correctamente (50 videos)\n",
      "\n",
      "Normal seleccionado para esta fase: Normal_Videos_event\n",
      "\n",
      "Subconjunto: LISTO para pasar a preprocesamiento y extracción de embeddings.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Paso 1.6 — Subconjunto de trabajo (Fase 1) ===\")\n",
    "\n",
    "SUBSET = [\"Abuse\", \"Arrest\", \"Arson\", \"Assault\"]\n",
    "\n",
    "subset_infos = []\n",
    "subset_ok = True\n",
    "\n",
    "for cls in SUBSET:\n",
    "    info = check_folder(cls, VIDEOS_DIR / cls, required=True)\n",
    "    subset_infos.append(info)\n",
    "    print_status(info)\n",
    "    subset_ok = subset_ok and (info[\"status\"] == \"OK\")\n",
    "\n",
    "# Elegir normal (prioriza oficial si existe; si no, placeholder)\n",
    "normal_choice = None\n",
    "for candidate in [\"Training_Normal_Videos_Anomaly\", \"Normal_Videos_event\"]:\n",
    "    info = check_folder(candidate, VIDEOS_DIR / candidate, required=False)\n",
    "    if info[\"status\"] == \"OK\":\n",
    "        normal_choice = candidate\n",
    "        print(f\"\\nNormal seleccionado para esta fase: {normal_choice}\")\n",
    "        break\n",
    "\n",
    "if subset_ok and normal_choice is not None:\n",
    "    print(\"\\nSubconjunto: LISTO para pasar a preprocesamiento y extracción de embeddings.\")\n",
    "else:\n",
    "    print(\"\\nSubconjunto: NO listo. Revisa carpetas faltantes/vacías o agrega una carpeta de normales.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd84dc31",
   "metadata": {},
   "source": [
    "## Paso 1.7 — Guardar reporte como evidencia (Código)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "855128c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reporte guardado en: UCF_Crime/report_dataset_validation.json\n"
     ]
    }
   ],
   "source": [
    "output_path = DATASET_ROOT / \"report_dataset_validation.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nReporte guardado en: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f72ddc8",
   "metadata": {},
   "source": [
    "# Paso 2 — Generación de splits (train/val/test) y etiquetas\n",
    "\n",
    "## Objetivo\n",
    "Construir un índice reproducible de videos y generar particiones `train/val/test` para una formulación binaria:\n",
    "- **1 = anomalía** (todas las clases anómalas)\n",
    "- **0 = normal**\n",
    "\n",
    "En esta fase se trabaja con el subconjunto:\n",
    "- Anomalías: Abuse, Arrest, Arson, Assault\n",
    "- Normal: Normal_Videos_event (placeholder)\n",
    "\n",
    "Se guardan:\n",
    "- `metadata.csv`\n",
    "- `splits/train.txt`, `splits/val.txt`, `splits/test.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67999e9",
   "metadata": {},
   "source": [
    "## Paso 2.1 — Parámetros de split (Código)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bb567dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros OK: 0.7 0.15 0.15 SEED: 42\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Reproducibilidad\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# Subconjunto actual (Fase 1)\n",
    "ANOMALY_SUBSET = [\"Abuse\", \"Arrest\", \"Arson\", \"Assault\"]\n",
    "NORMAL_FOLDER = \"Normal_Videos_event\"  # placeholder actual\n",
    "\n",
    "# Proporciones de split\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO   = 0.15\n",
    "TEST_RATIO  = 0.15\n",
    "\n",
    "assert abs((TRAIN_RATIO + VAL_RATIO + TEST_RATIO) - 1.0) < 1e-9\n",
    "print(\"Parámetros OK:\", TRAIN_RATIO, VAL_RATIO, TEST_RATIO, \"SEED:\", SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d294b1",
   "metadata": {},
   "source": [
    "## Paso 2.2 — Construir índice de videos (Código)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c57013f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos indexados: 250\n",
      "original_class\n",
      "Abuse                  50\n",
      "Arrest                 50\n",
      "Arson                  50\n",
      "Assault                50\n",
      "Normal_Videos_event    50\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    200\n",
      "0     50\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>path</th>\n",
       "      <th>original_class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abuse001_x264</td>\n",
       "      <td>Videos/Abuse/Abuse001_x264.mp4</td>\n",
       "      <td>Abuse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abuse002_x264</td>\n",
       "      <td>Videos/Abuse/Abuse002_x264.mp4</td>\n",
       "      <td>Abuse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abuse003_x264</td>\n",
       "      <td>Videos/Abuse/Abuse003_x264.mp4</td>\n",
       "      <td>Abuse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abuse004_x264</td>\n",
       "      <td>Videos/Abuse/Abuse004_x264.mp4</td>\n",
       "      <td>Abuse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abuse005_x264</td>\n",
       "      <td>Videos/Abuse/Abuse005_x264.mp4</td>\n",
       "      <td>Abuse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        video_id                            path original_class  label\n",
       "0  Abuse001_x264  Videos/Abuse/Abuse001_x264.mp4          Abuse      1\n",
       "1  Abuse002_x264  Videos/Abuse/Abuse002_x264.mp4          Abuse      1\n",
       "2  Abuse003_x264  Videos/Abuse/Abuse003_x264.mp4          Abuse      1\n",
       "3  Abuse004_x264  Videos/Abuse/Abuse004_x264.mp4          Abuse      1\n",
       "4  Abuse005_x264  Videos/Abuse/Abuse005_x264.mp4          Abuse      1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def collect_videos_from_class(class_name: str, label: int):\n",
    "    folder = VIDEOS_DIR / class_name\n",
    "    vids = list_videos(folder)\n",
    "    rows = []\n",
    "    for v in vids:\n",
    "        rows.append({\n",
    "            \"video_id\": v.stem,\n",
    "            \"path\": str(v.relative_to(DATASET_ROOT).as_posix()),  # path relativo (portable)\n",
    "            \"original_class\": class_name,\n",
    "            \"label\": label\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Anomalías -> label 1\n",
    "for cls in ANOMALY_SUBSET:\n",
    "    rows.extend(collect_videos_from_class(cls, label=1))\n",
    "\n",
    "# Normal -> label 0\n",
    "rows.extend(collect_videos_from_class(NORMAL_FOLDER, label=0))\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(\"Total videos indexados:\", len(df))\n",
    "print(df[\"original_class\"].value_counts())\n",
    "print(df[\"label\"].value_counts())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46dd20",
   "metadata": {},
   "source": [
    "## Paso 2.3 — Split estratificado por clase (recomendado para Fase 1) (Código)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd8a60aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaños: 175 40 35\n",
      "\n",
      "Train label counts:\n",
      " label\n",
      "1    140\n",
      "0     35\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Val label counts:\n",
      " label\n",
      "1    32\n",
      "0     8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test label counts:\n",
      " label\n",
      "1    28\n",
      "0     7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def stratified_split(df_in: pd.DataFrame, group_col: str):\n",
    "    train_parts, val_parts, test_parts = [], [], []\n",
    "\n",
    "    for group, gdf in df_in.groupby(group_col):\n",
    "        idx = list(gdf.index)\n",
    "        random.shuffle(idx)\n",
    "\n",
    "        n = len(idx)\n",
    "        n_train = int(round(n * TRAIN_RATIO))\n",
    "        n_val   = int(round(n * VAL_RATIO))\n",
    "        n_test  = n - n_train - n_val  # asegura suma exacta\n",
    "\n",
    "        train_idx = idx[:n_train]\n",
    "        val_idx   = idx[n_train:n_train+n_val]\n",
    "        test_idx  = idx[n_train+n_val:]\n",
    "\n",
    "        train_parts.append(df_in.loc[train_idx])\n",
    "        val_parts.append(df_in.loc[val_idx])\n",
    "        test_parts.append(df_in.loc[test_idx])\n",
    "\n",
    "    train_df = pd.concat(train_parts).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    val_df   = pd.concat(val_parts).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    test_df  = pd.concat(test_parts).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = stratified_split(df, group_col=\"original_class\")\n",
    "\n",
    "print(\"Tamaños:\", len(train_df), len(val_df), len(test_df))\n",
    "print(\"\\nTrain label counts:\\n\", train_df[\"label\"].value_counts())\n",
    "print(\"\\nVal label counts:\\n\", val_df[\"label\"].value_counts())\n",
    "print(\"\\nTest label counts:\\n\", test_df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce642ce",
   "metadata": {},
   "source": [
    "## Paso 2.4 — Validación del split (Código)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f3eafce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TRAIN ---\n",
      "Total: 175\n",
      "Por clase:\n",
      " original_class\n",
      "Normal_Videos_event    35\n",
      "Arson                  35\n",
      "Assault                35\n",
      "Arrest                 35\n",
      "Abuse                  35\n",
      "Name: count, dtype: int64\n",
      "Binario:\n",
      " label\n",
      "1    140\n",
      "0     35\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- VAL ---\n",
      "Total: 40\n",
      "Por clase:\n",
      " original_class\n",
      "Arson                  8\n",
      "Arrest                 8\n",
      "Assault                8\n",
      "Abuse                  8\n",
      "Normal_Videos_event    8\n",
      "Name: count, dtype: int64\n",
      "Binario:\n",
      " label\n",
      "1    32\n",
      "0     8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- TEST ---\n",
      "Total: 35\n",
      "Por clase:\n",
      " original_class\n",
      "Assault                7\n",
      "Arrest                 7\n",
      "Arson                  7\n",
      "Normal_Videos_event    7\n",
      "Abuse                  7\n",
      "Name: count, dtype: int64\n",
      "Binario:\n",
      " label\n",
      "1    28\n",
      "0     7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def split_report(name, sdf):\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(\"Total:\", len(sdf))\n",
    "    print(\"Por clase:\\n\", sdf[\"original_class\"].value_counts())\n",
    "    print(\"Binario:\\n\", sdf[\"label\"].value_counts())\n",
    "\n",
    "split_report(\"TRAIN\", train_df)\n",
    "split_report(\"VAL\", val_df)\n",
    "split_report(\"TEST\", test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2ca64b",
   "metadata": {},
   "source": [
    "## Paso 2.5 — Guardar metadata.csv y archivos splits/*.txt (Código)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "131ca2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: UCF_Crime/metadata.csv\n",
      "Splits guardados en: UCF_Crime/splits\n",
      " - UCF_Crime/splits/train.txt\n",
      " - UCF_Crime/splits/val.txt\n",
      " - UCF_Crime/splits/test.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "OUT_META = DATASET_ROOT / \"metadata.csv\"\n",
    "SPLITS_DIR = DATASET_ROOT / \"splits\"\n",
    "SPLITS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Guardar metadata completa\n",
    "df.to_csv(OUT_META, index=False)\n",
    "print(\"Guardado:\", OUT_META)\n",
    "\n",
    "# Guardar splits como listas de paths (uno por línea)\n",
    "(train_df[\"path\"]).to_csv(SPLITS_DIR / \"train.txt\", index=False, header=False)\n",
    "(val_df[\"path\"]).to_csv(SPLITS_DIR / \"val.txt\",   index=False, header=False)\n",
    "(test_df[\"path\"]).to_csv(SPLITS_DIR / \"test.txt\",  index=False, header=False)\n",
    "\n",
    "print(\"Splits guardados en:\", SPLITS_DIR)\n",
    "print(\" -\", SPLITS_DIR / \"train.txt\")\n",
    "print(\" -\", SPLITS_DIR / \"val.txt\")\n",
    "print(\" -\", SPLITS_DIR / \"test.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc55cb77",
   "metadata": {},
   "source": [
    "## Evidencia generada\n",
    "- `metadata.csv`: índice completo de videos con clase original y etiqueta binaria.\n",
    "- `splits/train.txt`, `splits/val.txt`, `splits/test.txt`: particiones reproducibles basadas en paths relativos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85109772",
   "metadata": {},
   "source": [
    "# Parte 3 — Preprocesamiento por split (train/val/test)\n",
    "\n",
    "## Objetivo\n",
    "Convertir cada video listado en `splits/train.txt`, `splits/val.txt`, `splits/test.txt` en un clip estándar de `N_FRAMES` frames, respetando los splits para evitar fuga de información.\n",
    "\n",
    "## Decisiones (Fase 1)\n",
    "- FPS objetivo: 30 (según README del dataset)\n",
    "- Clip por video: 1 (representativo)\n",
    "- Frames por clip: `N_FRAMES` (ej. 32)\n",
    "- Muestreo: uniforme en la duración del video (evita sesgo por inicio/final)\n",
    "\n",
    "## Salidas\n",
    "- `processed/<split>/frames/<video_id>/frame_0001.jpg ...`\n",
    "- `processed/preprocess_index.csv` (evidencia reproducible)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d275c88f",
   "metadata": {},
   "source": [
    "## Celda 3.1 — Parámetros y carpetas de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f46bba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSED_DIR: UCF_Crime/processed\n",
      "FPS_TARGET: 30 N_FRAMES: 32\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Parámetros fase 1\n",
    "FPS_TARGET = 30\n",
    "N_FRAMES = 32\n",
    "\n",
    "PROCESSED_DIR = DATASET_ROOT / \"processed\"\n",
    "for sp in [\"train\", \"val\", \"test\"]:\n",
    "    (PROCESSED_DIR / sp / \"frames\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROCESSED_DIR:\", PROCESSED_DIR)\n",
    "print(\"FPS_TARGET:\", FPS_TARGET, \"N_FRAMES:\", N_FRAMES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02af718a",
   "metadata": {},
   "source": [
    "## Celda 3.2 — Cargar splits (train/val/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0b62815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 175, 'val': 40, 'test': 35}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_split_paths(split_name: str):\n",
    "    p = DATASET_ROOT / \"splits\" / f\"{split_name}.txt\"\n",
    "    paths = [line.strip() for line in p.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "    return paths\n",
    "\n",
    "split_paths = {sp: load_split_paths(sp) for sp in [\"train\", \"val\", \"test\"]}\n",
    "{k: len(v) for k, v in split_paths.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323bdd09",
   "metadata": {},
   "source": [
    "## Celda 3.3 — Backend de lectura de video (OpenCV) + verificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "994c5a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV disponible: True\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import cv2\n",
    "    cv2_available = True\n",
    "except Exception as e:\n",
    "    cv2_available = False\n",
    "    cv2_err = str(e)\n",
    "\n",
    "print(\"OpenCV disponible:\", cv2_available)\n",
    "if not cv2_available:\n",
    "    print(\"Error OpenCV:\", cv2_err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f77623f",
   "metadata": {},
   "source": [
    "## Celda 3.4 — Verificar FFmpeg/FFprobe (fallback técnico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98b4d302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFmpeg/FFprobe disponible: True\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def ffmpeg_exists():\n",
    "    try:\n",
    "        subprocess.run([\"ffmpeg\", \"-version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n",
    "        subprocess.run([\"ffprobe\", \"-version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(\"FFmpeg/FFprobe disponible:\", ffmpeg_exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933f7c2e",
   "metadata": {},
   "source": [
    "## Celda 3.5 — Helpers de rutas e ID reproducible por video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4aba35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_video_abs(rel_path: str) -> Path:\n",
    "    # split guarda rutas relativas a DATASET_ROOT\n",
    "    return (DATASET_ROOT / rel_path).resolve()\n",
    "\n",
    "def make_video_id(rel_path: str) -> str:\n",
    "    # ID reproducible basado en la ruta (sin extensión)\n",
    "    p = Path(rel_path)\n",
    "    no_ext = p.with_suffix(\"\")\n",
    "    return \"__\".join(no_ext.parts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bdf8fc",
   "metadata": {},
   "source": [
    "## Celda 3.6 — Funciones OpenCV: muestreo uniforme por índice + guardado JPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2ea00ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones OpenCV listas.\n"
     ]
    }
   ],
   "source": [
    "def get_video_info_cv2(video_path: Path):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        cap.release()\n",
    "        return None\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    cap.release()\n",
    "    if frame_count <= 0:\n",
    "        return None\n",
    "    return {\"frame_count\": frame_count, \"fps\": float(fps) if fps else None}\n",
    "\n",
    "def read_frame_at_index_cv2(video_path: Path, idx: int):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        cap.release()\n",
    "        return None\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "    ok, frame_bgr = cap.read()\n",
    "    cap.release()\n",
    "    if (not ok) or (frame_bgr is None):\n",
    "        return None\n",
    "    return frame_bgr  # BGR\n",
    "\n",
    "def extract_uniform_clip_cv2(video_path: Path, n_frames: int):\n",
    "    info = get_video_info_cv2(video_path)\n",
    "    if info is None:\n",
    "        return None, {\"ok\": False, \"reason\": \"cannot_open_or_probe\"}\n",
    "\n",
    "    T = info[\"frame_count\"]\n",
    "    if T < 2:\n",
    "        return None, {\"ok\": False, \"reason\": \"too_few_frames\", \"frame_count\": T}\n",
    "\n",
    "    indices = np.linspace(0, T - 1, n_frames).astype(int).tolist()\n",
    "\n",
    "    frames_bgr = []\n",
    "    for idx in indices:\n",
    "        fr = read_frame_at_index_cv2(video_path, idx)\n",
    "        if fr is None:\n",
    "            return None, {\n",
    "                \"ok\": False,\n",
    "                \"reason\": \"frame_read_failed\",\n",
    "                \"failed_index\": idx,\n",
    "                \"frame_count\": T,\n",
    "                \"source_fps\": info[\"fps\"],\n",
    "                \"indices\": indices\n",
    "            }\n",
    "        frames_bgr.append(fr)\n",
    "\n",
    "    return frames_bgr, {\n",
    "        \"ok\": True,\n",
    "        \"frame_count\": T,\n",
    "        \"source_fps\": info[\"fps\"],\n",
    "        \"indices\": indices\n",
    "    }\n",
    "\n",
    "def save_frames_as_jpg_bgr(frames_bgr, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    saved = 0\n",
    "    for i, fr_bgr in enumerate(frames_bgr, start=1):\n",
    "        out_path = out_dir / f\"frame_{i:04d}.jpg\"\n",
    "        if cv2.imwrite(str(out_path), fr_bgr):\n",
    "            saved += 1\n",
    "    return saved\n",
    "\n",
    "print(\"Funciones OpenCV listas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717576ff",
   "metadata": {},
   "source": [
    "## Celda 3.7 — Fallback FFmpeg: extracción simple (solo si OpenCV falla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e231c6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallback FFmpeg listo.\n"
     ]
    }
   ],
   "source": [
    "def extract_frames_ffmpeg_fallback(video_abs: Path, out_dir: Path, n_frames: int, fps: int):\n",
    "    \"\"\"\n",
    "    Fallback simple: extrae n_frames a fps desde el inicio, clip de duración n_frames/fps.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_pattern = str(out_dir / \"frame_%04d.jpg\")\n",
    "    clip_seconds = n_frames / fps\n",
    "\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-y\",\n",
    "        \"-i\", str(video_abs),\n",
    "        \"-vf\", f\"fps={fps}\",\n",
    "        \"-t\", f\"{clip_seconds:.3f}\",\n",
    "        \"-vframes\", str(n_frames),\n",
    "        \"-q:v\", \"2\",\n",
    "        out_pattern\n",
    "    ]\n",
    "    proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    if proc.returncode != 0:\n",
    "        return {\"ok\": False, \"reason\": \"ffmpeg_failed\", \"stderr\": proc.stderr[-400:]}\n",
    "\n",
    "    frames = sorted(out_dir.glob(\"frame_*.jpg\"))\n",
    "    if len(frames) < n_frames:\n",
    "        return {\"ok\": False, \"reason\": \"insufficient_frames\", \"extracted\": len(frames)}\n",
    "\n",
    "    if len(frames) > n_frames:\n",
    "        for f in frames[n_frames:]:\n",
    "            f.unlink()\n",
    "\n",
    "    return {\"ok\": True, \"extracted\": n_frames}\n",
    "\n",
    "print(\"Fallback FFmpeg listo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c3067c",
   "metadata": {},
   "source": [
    "## Celda 3.8 — Preprocesamiento por split (train/val/test) + registro reproducible (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a47d95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Preprocesando train | videos: 175 ==\n",
      "\n",
      "== Preprocesando val | videos: 40 ==\n",
      "\n",
      "== Preprocesando test | videos: 35 ==\n",
      "\n",
      "Listo. CSV: UCF_Crime/processed/preprocess_index.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_ts</th>\n",
       "      <th>split</th>\n",
       "      <th>rel_path</th>\n",
       "      <th>video_abs</th>\n",
       "      <th>video_id</th>\n",
       "      <th>method</th>\n",
       "      <th>status</th>\n",
       "      <th>saved</th>\n",
       "      <th>source_fps</th>\n",
       "      <th>frame_count</th>\n",
       "      <th>failed_index</th>\n",
       "      <th>reason</th>\n",
       "      <th>stderr_tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-04T16:38:03</td>\n",
       "      <td>train</td>\n",
       "      <td>Videos/Normal_Videos_event/Normal_Videos_597_x...</td>\n",
       "      <td>/home/diego/Escritorio/Pruebas/tesispython/UCF...</td>\n",
       "      <td>Videos__Normal_Videos_event__Normal_Videos_597...</td>\n",
       "      <td>skip_exists</td>\n",
       "      <td>ok</td>\n",
       "      <td>32</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-01-04T16:38:03</td>\n",
       "      <td>train</td>\n",
       "      <td>Videos/Normal_Videos_event/Normal_Videos_603_x...</td>\n",
       "      <td>/home/diego/Escritorio/Pruebas/tesispython/UCF...</td>\n",
       "      <td>Videos__Normal_Videos_event__Normal_Videos_603...</td>\n",
       "      <td>skip_exists</td>\n",
       "      <td>ok</td>\n",
       "      <td>32</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-01-04T16:38:03</td>\n",
       "      <td>train</td>\n",
       "      <td>Videos/Arson/Arson046_x264.mp4</td>\n",
       "      <td>/home/diego/Escritorio/Pruebas/tesispython/UCF...</td>\n",
       "      <td>Videos__Arson__Arson046_x264</td>\n",
       "      <td>skip_exists</td>\n",
       "      <td>ok</td>\n",
       "      <td>32</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-01-04T16:38:03</td>\n",
       "      <td>train</td>\n",
       "      <td>Videos/Assault/Assault002_x264.mp4</td>\n",
       "      <td>/home/diego/Escritorio/Pruebas/tesispython/UCF...</td>\n",
       "      <td>Videos__Assault__Assault002_x264</td>\n",
       "      <td>skip_exists</td>\n",
       "      <td>ok</td>\n",
       "      <td>32</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-01-04T16:38:03</td>\n",
       "      <td>train</td>\n",
       "      <td>Videos/Normal_Videos_event/Normal_Videos_656_x...</td>\n",
       "      <td>/home/diego/Escritorio/Pruebas/tesispython/UCF...</td>\n",
       "      <td>Videos__Normal_Videos_event__Normal_Videos_656...</td>\n",
       "      <td>skip_exists</td>\n",
       "      <td>ok</td>\n",
       "      <td>32</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                run_ts  split  \\\n",
       "0  2026-01-04T16:38:03  train   \n",
       "1  2026-01-04T16:38:03  train   \n",
       "2  2026-01-04T16:38:03  train   \n",
       "3  2026-01-04T16:38:03  train   \n",
       "4  2026-01-04T16:38:03  train   \n",
       "\n",
       "                                            rel_path  \\\n",
       "0  Videos/Normal_Videos_event/Normal_Videos_597_x...   \n",
       "1  Videos/Normal_Videos_event/Normal_Videos_603_x...   \n",
       "2                     Videos/Arson/Arson046_x264.mp4   \n",
       "3                 Videos/Assault/Assault002_x264.mp4   \n",
       "4  Videos/Normal_Videos_event/Normal_Videos_656_x...   \n",
       "\n",
       "                                           video_abs  \\\n",
       "0  /home/diego/Escritorio/Pruebas/tesispython/UCF...   \n",
       "1  /home/diego/Escritorio/Pruebas/tesispython/UCF...   \n",
       "2  /home/diego/Escritorio/Pruebas/tesispython/UCF...   \n",
       "3  /home/diego/Escritorio/Pruebas/tesispython/UCF...   \n",
       "4  /home/diego/Escritorio/Pruebas/tesispython/UCF...   \n",
       "\n",
       "                                            video_id       method status  \\\n",
       "0  Videos__Normal_Videos_event__Normal_Videos_597...  skip_exists     ok   \n",
       "1  Videos__Normal_Videos_event__Normal_Videos_603...  skip_exists     ok   \n",
       "2                       Videos__Arson__Arson046_x264  skip_exists     ok   \n",
       "3                   Videos__Assault__Assault002_x264  skip_exists     ok   \n",
       "4  Videos__Normal_Videos_event__Normal_Videos_656...  skip_exists     ok   \n",
       "\n",
       "   saved source_fps frame_count failed_index reason stderr_tail  \n",
       "0     32       None        None         None   None        None  \n",
       "1     32       None        None         None   None        None  \n",
       "2     32       None        None         None   None        None  \n",
       "3     32       None        None         None   None        None  \n",
       "4     32       None        None         None   None        None  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "run_ts = datetime.now().isoformat(timespec=\"seconds\")\n",
    "\n",
    "for split_name in [\"train\", \"val\", \"test\"]:\n",
    "    rel_list = split_paths[split_name]\n",
    "    print(f\"\\n== Preprocesando {split_name} | videos: {len(rel_list)} ==\")\n",
    "\n",
    "    for i, rel_path in enumerate(rel_list, start=1):\n",
    "        video_abs = resolve_video_abs(rel_path)\n",
    "        video_id = make_video_id(rel_path)\n",
    "        out_dir = PROCESSED_DIR / split_name / \"frames\" / video_id\n",
    "\n",
    "        # 1) Skip si ya existe completo\n",
    "        existing = sorted(out_dir.glob(\"frame_*.jpg\"))\n",
    "        if len(existing) >= N_FRAMES:\n",
    "            rows.append({\n",
    "                \"run_ts\": run_ts,\n",
    "                \"split\": split_name,\n",
    "                \"rel_path\": rel_path,\n",
    "                \"video_abs\": str(video_abs),\n",
    "                \"video_id\": video_id,\n",
    "                \"method\": \"skip_exists\",\n",
    "                \"status\": \"ok\",\n",
    "                \"saved\": len(existing),\n",
    "                \"source_fps\": None,\n",
    "                \"frame_count\": None,\n",
    "                \"failed_index\": None,\n",
    "                \"reason\": None,\n",
    "                \"stderr_tail\": None,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # 2) Verificar existencia del archivo\n",
    "        if not video_abs.exists():\n",
    "            rows.append({\n",
    "                \"run_ts\": run_ts,\n",
    "                \"split\": split_name,\n",
    "                \"rel_path\": rel_path,\n",
    "                \"video_abs\": str(video_abs),\n",
    "                \"video_id\": video_id,\n",
    "                \"method\": \"none\",\n",
    "                \"status\": \"fail\",\n",
    "                \"saved\": 0,\n",
    "                \"source_fps\": None,\n",
    "                \"frame_count\": None,\n",
    "                \"failed_index\": None,\n",
    "                \"reason\": \"missing_file\",\n",
    "                \"stderr_tail\": None,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # 3) Intento OpenCV (uniforme por índices)\n",
    "        frames_bgr, meta = extract_uniform_clip_cv2(video_abs, N_FRAMES)\n",
    "        if meta[\"ok\"]:\n",
    "            saved = save_frames_as_jpg_bgr(frames_bgr, out_dir)\n",
    "            status = \"ok\" if saved == N_FRAMES else \"fail\"\n",
    "\n",
    "            rows.append({\n",
    "                \"run_ts\": run_ts,\n",
    "                \"split\": split_name,\n",
    "                \"rel_path\": rel_path,\n",
    "                \"video_abs\": str(video_abs),\n",
    "                \"video_id\": video_id,\n",
    "                \"method\": \"opencv_uniform\",\n",
    "                \"status\": status,\n",
    "                \"saved\": saved,\n",
    "                \"source_fps\": meta.get(\"source_fps\"),\n",
    "                \"frame_count\": meta.get(\"frame_count\"),\n",
    "                \"failed_index\": None,\n",
    "                \"reason\": None if status == \"ok\" else \"save_failed\",\n",
    "                \"stderr_tail\": None,\n",
    "            })\n",
    "\n",
    "        else:\n",
    "            # 4) Fallback FFmpeg si OpenCV falla\n",
    "            fb = extract_frames_ffmpeg_fallback(video_abs, out_dir, N_FRAMES, FPS_TARGET)\n",
    "            rows.append({\n",
    "                \"run_ts\": run_ts,\n",
    "                \"split\": split_name,\n",
    "                \"rel_path\": rel_path,\n",
    "                \"video_abs\": str(video_abs),\n",
    "                \"video_id\": video_id,\n",
    "                \"method\": \"ffmpeg_fallback\",\n",
    "                \"status\": \"ok\" if fb.get(\"ok\") else \"fail\",\n",
    "                \"saved\": fb.get(\"extracted\", 0),\n",
    "                \"source_fps\": meta.get(\"source_fps\"),\n",
    "                \"frame_count\": meta.get(\"frame_count\"),\n",
    "                \"failed_index\": meta.get(\"failed_index\"),\n",
    "                \"reason\": meta.get(\"reason\") if not fb.get(\"ok\") else \"opencv_failed_used_fallback\",\n",
    "                \"stderr_tail\": fb.get(\"stderr\"),\n",
    "            })\n",
    "\n",
    "        if i % 25 == 0:\n",
    "            print(f\"  Progreso {split_name}: {i}/{len(rel_list)}\")\n",
    "\n",
    "index_df = pd.DataFrame(rows)\n",
    "index_csv = PROCESSED_DIR / \"preprocess_index.csv\"\n",
    "index_df.to_csv(index_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\nListo. CSV:\", index_csv)\n",
    "index_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d8b4a6",
   "metadata": {},
   "source": [
    "## Celda 3.9 — Validación rápida (sanity checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f0bc00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>method</th>\n",
       "      <th>status</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>skip_exists</td>\n",
       "      <td>ok</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>skip_exists</td>\n",
       "      <td>ok</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>val</td>\n",
       "      <td>skip_exists</td>\n",
       "      <td>ok</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split       method status  count\n",
       "0   test  skip_exists     ok     35\n",
       "1  train  skip_exists     ok    175\n",
       "2    val  skip_exists     ok     40"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resumen por split / método / estado\n",
    "summary = index_df.groupby([\"split\", \"method\", \"status\"]).size().reset_index(name=\"count\")\n",
    "summary.sort_values([\"split\", \"status\", \"method\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d76204",
   "metadata": {},
   "source": [
    "## Celda 3.10 — Inspeccionar fallos (si existen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64dd0964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallos: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_ts</th>\n",
       "      <th>split</th>\n",
       "      <th>rel_path</th>\n",
       "      <th>video_abs</th>\n",
       "      <th>video_id</th>\n",
       "      <th>method</th>\n",
       "      <th>status</th>\n",
       "      <th>saved</th>\n",
       "      <th>source_fps</th>\n",
       "      <th>frame_count</th>\n",
       "      <th>failed_index</th>\n",
       "      <th>reason</th>\n",
       "      <th>stderr_tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [run_ts, split, rel_path, video_abs, video_id, method, status, saved, source_fps, frame_count, failed_index, reason, stderr_tail]\n",
       "Index: []"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad = index_df[index_df[\"status\"] != \"ok\"]\n",
    "print(\"Fallos:\", len(bad))\n",
    "bad.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bc285b",
   "metadata": {},
   "source": [
    "## Celda 3.11 — Verificar que un par de carpetas tengan N_FRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e306e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Videos__Normal_Videos_event__Normal_Videos_597_x264 frames: 32\n",
      "train Videos__Normal_Videos_event__Normal_Videos_603_x264 frames: 32\n",
      "train Videos__Arson__Arson046_x264 frames: 32\n",
      "train Videos__Assault__Assault002_x264 frames: 32\n",
      "train Videos__Normal_Videos_event__Normal_Videos_656_x264 frames: 32\n"
     ]
    }
   ],
   "source": [
    "def count_frames_in_dir(d: Path):\n",
    "    return len(list(d.glob(\"frame_*.jpg\")))\n",
    "\n",
    "sample_ok = index_df[index_df[\"status\"]==\"ok\"].head(5)\n",
    "for _, r in sample_ok.iterrows():\n",
    "    d = PROCESSED_DIR / r[\"split\"] / \"frames\" / r[\"video_id\"]\n",
    "    print(r[\"split\"], r[\"video_id\"], \"frames:\", count_frames_in_dir(d))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7ebe82",
   "metadata": {},
   "source": [
    "# Parte 4 — Dataset (clips desde frames) + preparación para Codificador/LoRA\n",
    "\n",
    "En esta parte conectamos el preprocesamiento con el modelo:\n",
    "\n",
    "1. Definimos configuración e imports.\n",
    "2. Construimos un `Dataset` que carga **clips de 32 frames** desde `processed/{split}/frames/{video_id}`.\n",
    "3. Cargamos el índice maestro (`processed/preprocess_index.csv`) y los splits (`splits/train.txt`).\n",
    "4. Normalizamos IDs para que los splits coincidan con el `video_id` interno.\n",
    "5. Inferimos `label` binario (normal vs anomalía) para VAD.\n",
    "6. Hacemos un sanity check: shapes y etiqueta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04e0bfd",
   "metadata": {},
   "source": [
    "## 4.1 Imports y configuración global\n",
    "\n",
    "Definimos el dispositivo, tamaño del clip (número de frames) y rutas base del proyecto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de02bd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n",
      "DATASET_ROOT: /home/diego/Escritorio/Pruebas/tesispython/UCF_Crime exists: True\n",
      "PROCESSED_DIR: /home/diego/Escritorio/Pruebas/tesispython/UCF_Crime/processed exists: True\n",
      "Index exists: True\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 4.1 — Imports y Config\n",
    "# ================================\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Usando dispositivo:\", DEVICE)\n",
    "\n",
    "# Cantidad de frames por clip (debe coincidir con tu Parte 3)\n",
    "N_FRAMES = 32\n",
    "\n",
    "# Rutas (ajustadas a tu estructura real)\n",
    "NOTEBOOK_DIR = Path(\"/home/diego/Escritorio/Pruebas/tesispython\")\n",
    "DATASET_ROOT = NOTEBOOK_DIR / \"UCF_Crime\"\n",
    "PROCESSED_DIR = DATASET_ROOT / \"processed\"\n",
    "\n",
    "print(\"DATASET_ROOT:\", DATASET_ROOT, \"exists:\", DATASET_ROOT.exists())\n",
    "print(\"PROCESSED_DIR:\", PROCESSED_DIR, \"exists:\", PROCESSED_DIR.exists())\n",
    "print(\"Index exists:\", (PROCESSED_DIR / \"preprocess_index.csv\").exists())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe705893",
   "metadata": {},
   "source": [
    "## 4.2 Dataset de frames (clip de T frames)\n",
    "\n",
    "Este `Dataset` carga un clip de `N_FRAMES` imágenes desde:\n",
    "\n",
    "`processed/{split}/frames/{video_id}/frame_0001.jpg ... frame_0032.jpg`\n",
    "\n",
    "Devuelve:\n",
    "- `video_id`\n",
    "- `clip`: tensor `[T, C, H, W]`\n",
    "- `label`: entero (0 normal, 1 anomalía)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a92084eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Parte 4.2 — Dataset de Frames\n",
    "# ================================\n",
    "class FrameClipDataset(Dataset):\n",
    "    def __init__(self, df, processed_dir, split, n_frames=32, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.processed_dir = processed_dir\n",
    "        self.split = split\n",
    "        self.n_frames = n_frames\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        video_id = row[\"video_id\"]\n",
    "        label = int(row[\"label\"]) if \"label\" in row else -1\n",
    "\n",
    "        frames_dir = self.processed_dir / self.split / \"frames\" / video_id\n",
    "        frame_paths = [frames_dir / f\"frame_{i:04d}.jpg\" for i in range(1, self.n_frames + 1)]\n",
    "\n",
    "        frames = []\n",
    "        for p in frame_paths:\n",
    "            img = Image.open(p).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            frames.append(img)\n",
    "\n",
    "        clip = torch.stack(frames, dim=0)  # [T, C, H, W]\n",
    "        return {\"video_id\": video_id, \"clip\": clip, \"label\": label}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2558bdd6",
   "metadata": {},
   "source": [
    "## 4.3 Construcción del `train_df` desde índice + splits\n",
    "\n",
    "- Cargamos el índice maestro desde: `processed/preprocess_index.csv`\n",
    "- Cargamos el split desde: `splits/train.txt`\n",
    "- Normalizamos los IDs del split para que coincidan con el formato interno `video_id` del índice:\n",
    "\n",
    "Split original:\n",
    "`Videos/Arrest/Arrest001_x264.mp4`\n",
    "\n",
    "Formato interno:\n",
    "`Videos__Arrest__Arrest001_x264`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b2fccaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index cargado: (250, 13)\n",
      "Train IDs: 175\n",
      "Ejemplo normalizado: ['Videos__Normal_Videos_event__Normal_Videos_597_x264', 'Videos__Normal_Videos_event__Normal_Videos_603_x264', 'Videos__Arson__Arson046_x264', 'Videos__Assault__Assault002_x264', 'Videos__Normal_Videos_event__Normal_Videos_656_x264']\n",
      "Train DF: (175, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_ts</th>\n",
       "      <th>split</th>\n",
       "      <th>rel_path</th>\n",
       "      <th>video_abs</th>\n",
       "      <th>video_id</th>\n",
       "      <th>method</th>\n",
       "      <th>status</th>\n",
       "      <th>saved</th>\n",
       "      <th>source_fps</th>\n",
       "      <th>frame_count</th>\n",
       "      <th>failed_index</th>\n",
       "      <th>reason</th>\n",
       "      <th>stderr_tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-04T16:38:03</td>\n",
       "      <td>train</td>\n",
       "      <td>Videos/Normal_Videos_event/Normal_Videos_597_x...</td>\n",
       "      <td>/home/diego/Escritorio/Pruebas/tesispython/UCF...</td>\n",
       "      <td>Videos__Normal_Videos_event__Normal_Videos_597...</td>\n",
       "      <td>skip_exists</td>\n",
       "      <td>ok</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-01-04T16:38:03</td>\n",
       "      <td>train</td>\n",
       "      <td>Videos/Normal_Videos_event/Normal_Videos_603_x...</td>\n",
       "      <td>/home/diego/Escritorio/Pruebas/tesispython/UCF...</td>\n",
       "      <td>Videos__Normal_Videos_event__Normal_Videos_603...</td>\n",
       "      <td>skip_exists</td>\n",
       "      <td>ok</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-01-04T16:38:03</td>\n",
       "      <td>train</td>\n",
       "      <td>Videos/Arson/Arson046_x264.mp4</td>\n",
       "      <td>/home/diego/Escritorio/Pruebas/tesispython/UCF...</td>\n",
       "      <td>Videos__Arson__Arson046_x264</td>\n",
       "      <td>skip_exists</td>\n",
       "      <td>ok</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-01-04T16:38:03</td>\n",
       "      <td>train</td>\n",
       "      <td>Videos/Assault/Assault002_x264.mp4</td>\n",
       "      <td>/home/diego/Escritorio/Pruebas/tesispython/UCF...</td>\n",
       "      <td>Videos__Assault__Assault002_x264</td>\n",
       "      <td>skip_exists</td>\n",
       "      <td>ok</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-01-04T16:38:03</td>\n",
       "      <td>train</td>\n",
       "      <td>Videos/Normal_Videos_event/Normal_Videos_656_x...</td>\n",
       "      <td>/home/diego/Escritorio/Pruebas/tesispython/UCF...</td>\n",
       "      <td>Videos__Normal_Videos_event__Normal_Videos_656...</td>\n",
       "      <td>skip_exists</td>\n",
       "      <td>ok</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                run_ts  split  \\\n",
       "0  2026-01-04T16:38:03  train   \n",
       "1  2026-01-04T16:38:03  train   \n",
       "2  2026-01-04T16:38:03  train   \n",
       "3  2026-01-04T16:38:03  train   \n",
       "4  2026-01-04T16:38:03  train   \n",
       "\n",
       "                                            rel_path  \\\n",
       "0  Videos/Normal_Videos_event/Normal_Videos_597_x...   \n",
       "1  Videos/Normal_Videos_event/Normal_Videos_603_x...   \n",
       "2                     Videos/Arson/Arson046_x264.mp4   \n",
       "3                 Videos/Assault/Assault002_x264.mp4   \n",
       "4  Videos/Normal_Videos_event/Normal_Videos_656_x...   \n",
       "\n",
       "                                           video_abs  \\\n",
       "0  /home/diego/Escritorio/Pruebas/tesispython/UCF...   \n",
       "1  /home/diego/Escritorio/Pruebas/tesispython/UCF...   \n",
       "2  /home/diego/Escritorio/Pruebas/tesispython/UCF...   \n",
       "3  /home/diego/Escritorio/Pruebas/tesispython/UCF...   \n",
       "4  /home/diego/Escritorio/Pruebas/tesispython/UCF...   \n",
       "\n",
       "                                            video_id       method status  \\\n",
       "0  Videos__Normal_Videos_event__Normal_Videos_597...  skip_exists     ok   \n",
       "1  Videos__Normal_Videos_event__Normal_Videos_603...  skip_exists     ok   \n",
       "2                       Videos__Arson__Arson046_x264  skip_exists     ok   \n",
       "3                   Videos__Assault__Assault002_x264  skip_exists     ok   \n",
       "4  Videos__Normal_Videos_event__Normal_Videos_656...  skip_exists     ok   \n",
       "\n",
       "   saved  source_fps  frame_count  failed_index  reason  stderr_tail  \n",
       "0     32         NaN          NaN           NaN     NaN          NaN  \n",
       "1     32         NaN          NaN           NaN     NaN          NaN  \n",
       "2     32         NaN          NaN           NaN     NaN          NaN  \n",
       "3     32         NaN          NaN           NaN     NaN          NaN  \n",
       "4     32         NaN          NaN           NaN     NaN          NaN  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 4.3 — Índice + split + normalización\n",
    "# ================================\n",
    "\n",
    "# 1) Índice maestro\n",
    "index_df = pd.read_csv(PROCESSED_DIR / \"preprocess_index.csv\")\n",
    "print(\"Index cargado:\", index_df.shape)\n",
    "\n",
    "# 2) Split train (txt)\n",
    "with open(DATASET_ROOT / \"splits\" / \"train.txt\") as f:\n",
    "    train_ids = [l.strip() for l in f if l.strip()]\n",
    "\n",
    "print(\"Train IDs:\", len(train_ids))\n",
    "\n",
    "# 3) Normalización de IDs (split path -> video_id interno)\n",
    "def split_path_to_index_id(p):\n",
    "    \"\"\"\n",
    "    Convierte:\n",
    "    Videos/Arrest/Arrest001_x264.mp4\n",
    "    →\n",
    "    Videos__Arrest__Arrest001_x264\n",
    "    \"\"\"\n",
    "    p = Path(p)\n",
    "    return f\"{p.parts[0]}__{p.parts[1]}__{p.stem}\"\n",
    "\n",
    "train_ids_norm = [split_path_to_index_id(x) for x in train_ids]\n",
    "print(\"Ejemplo normalizado:\", train_ids_norm[:5])\n",
    "\n",
    "# 4) Filtrar índice usando IDs normalizados\n",
    "train_df = index_df[index_df[\"video_id\"].isin(train_ids_norm)].reset_index(drop=True)\n",
    "print(\"Train DF:\", train_df.shape)\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c2f7e2",
   "metadata": {},
   "source": [
    "## 4.3.b Etiquetado binario (VAD)\n",
    "\n",
    "Convertimos UCF-Crime a **VAD binario**:\n",
    "\n",
    "- `Normal_Videos_event` → label = 0 (normal)\n",
    "- resto de clases → label = 1 (anomalía)\n",
    "\n",
    "La etiqueta se infiere desde el `video_id` interno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ac795fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de labels:\n",
      "label\n",
      "1    140\n",
      "0     35\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 4.3.b — Inferir label binario\n",
    "# ================================\n",
    "def infer_label_from_video_id(video_id):\n",
    "    if \"__Normal_Videos_event__\" in video_id:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "train_df[\"label\"] = train_df[\"video_id\"].apply(infer_label_from_video_id)\n",
    "\n",
    "print(\"Distribución de labels:\")\n",
    "print(train_df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e574cb",
   "metadata": {},
   "source": [
    "## 4.4 Transform y sanity check (shape, label)\n",
    "\n",
    "Definimos una transformación base para frames.\n",
    "En esta etapa solo validamos que:\n",
    "\n",
    "- el clip tiene shape `[32, 3, 384, 384]`\n",
    "- el label es 0 o 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6a2e5add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ID: Videos__Normal_Videos_event__Normal_Videos_597_x264\n",
      "Clip shape: torch.Size([32, 3, 384, 384])\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 4.4 — Transform + Dataset + Test\n",
    "# ================================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((384, 384)),  # SigLIP suele trabajar bien con 384\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = FrameClipDataset(\n",
    "    train_df,\n",
    "    PROCESSED_DIR,\n",
    "    split=\"train\",\n",
    "    n_frames=N_FRAMES,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "sample = dataset[0]\n",
    "print(\"Video ID:\", sample[\"video_id\"])\n",
    "print(\"Clip shape:\", sample[\"clip\"].shape)  # esperado: [32, 3, 384, 384]\n",
    "print(\"Label:\", sample[\"label\"])            # esperado: 0 o 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7257fe0f",
   "metadata": {},
   "source": [
    "## 4.5 Carga del codificador visual (SigLIP)\n",
    "\n",
    "Se utiliza SigLIP como codificador visual preentrenado.\n",
    "En esta etapa **NO se entrena** el modelo: solo se valida la extracción\n",
    "de embeddings a partir de clips de frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f81cdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SigLIP cargado correctamente\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 4.5 — Cargar SigLIP\n",
    "# ================================\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "SIGLIP_NAME = \"google/siglip-so400m-patch14-384\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(SIGLIP_NAME)\n",
    "encoder = AutoModel.from_pretrained(SIGLIP_NAME).to(DEVICE)\n",
    "encoder.eval()\n",
    "\n",
    "print(\"SigLIP cargado correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc3866",
   "metadata": {},
   "source": [
    "## 4.6 Extracción de embedding de video (sin LoRA)\n",
    "\n",
    "Cada frame del clip se procesa individualmente con SigLIP.\n",
    "Luego, los embeddings temporales se agregan mediante **average pooling**\n",
    "para obtener una representación única por video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9da921d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([1, 1152])\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 4.6 — Encoding del clip\n",
    "# ================================\n",
    "@torch.no_grad()\n",
    "def encode_clip_siglip(encoder, processor, clip):\n",
    "    \"\"\"\n",
    "    clip: torch.Tensor [T, C, H, W]\n",
    "    return: torch.Tensor [1, D]\n",
    "    \"\"\"\n",
    "    # Convertir frames a PIL\n",
    "    imgs = [transforms.ToPILImage()(clip[t]) for t in range(clip.shape[0])]\n",
    "\n",
    "    # Preprocesamiento SigLIP\n",
    "    inputs = processor(images=imgs, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    # Forward\n",
    "    feats = encoder.get_image_features(**inputs)  # [T, D]\n",
    "\n",
    "    # Pooling temporal\n",
    "    video_emb = feats.mean(dim=0, keepdim=True)   # [1, D]\n",
    "    return video_emb\n",
    "\n",
    "# Test con un sample\n",
    "emb = encode_clip_siglip(encoder, processor, sample[\"clip\"])\n",
    "print(\"Embedding shape:\", emb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a888c6a0",
   "metadata": {},
   "source": [
    "## 4.7 Congelamiento del codificador\n",
    "\n",
    "Antes de aplicar LoRA, se congelan todos los parámetros del codificador\n",
    "para asegurar que solo los adaptadores entrenables modifiquen el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0f2233be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder congelado\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 4.7 — Freeze del encoder\n",
    "# ================================\n",
    "for p in encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"Encoder congelado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ebd905",
   "metadata": {},
   "source": [
    "## 4.8 Adaptación del codificador mediante LoRA\n",
    "\n",
    "Se emplea LoRA (Low-Rank Adaptation) para adaptar de forma eficiente\n",
    "ciertas capas del mecanismo de atención, reduciendo el número de\n",
    "parámetros entrenables y el costo computacional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bcbb4830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros entrenables: 1,990,656\n",
      "Parámetros totales:    879,951,154\n",
      "Porcentaje entrenable: 0.2262%\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 4.8 — LoRA (PEFT)\n",
    "# ================================\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "encoder_lora = get_peft_model(encoder, lora_cfg).to(DEVICE)\n",
    "encoder_lora.train()\n",
    "\n",
    "# Contar parámetros\n",
    "trainable = sum(p.numel() for p in encoder_lora.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in encoder_lora.parameters())\n",
    "\n",
    "print(f\"Parámetros entrenables: {trainable:,}\")\n",
    "print(f\"Parámetros totales:    {total:,}\")\n",
    "print(f\"Porcentaje entrenable: {100 * trainable / total:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbea06d",
   "metadata": {},
   "source": [
    "## 4.9 Validación del forward con SigLIP + LoRA\n",
    "\n",
    "Se verifica que la incorporación de LoRA no altera la dimensionalidad\n",
    "del embedding y que el forward pass es estable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e40650b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding LoRA shape: torch.Size([1, 1152])\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 4.9 — Test forward con LoRA\n",
    "# ================================\n",
    "emb_lora = encode_clip_siglip(encoder_lora, processor, sample[\"clip\"])\n",
    "print(\"Embedding LoRA shape:\", emb_lora.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d462e4",
   "metadata": {},
   "source": [
    "# Parte 5 — Extracción de embeddings (Baseline)\n",
    "\n",
    "En esta etapa se extraen embeddings de video utilizando el codificador\n",
    "SigLIP **preentrenado y congelado**, sin adaptación adicional (LoRA).\n",
    "\n",
    "Estos embeddings constituyen el **baseline** del sistema y serán usados\n",
    "para entrenar y evaluar un clasificador MLP en la Parte 6.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd47a152",
   "metadata": {},
   "source": [
    "## 5.1 Construcción de DataFrames por split (train / val / test)\n",
    "\n",
    "Se reutiliza el índice maestro y los splits originales, normalizando IDs\n",
    "y asignando etiquetas binarias de VAD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "89b9fa05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (175, 14)\n",
      "val: (40, 14)\n",
      "test: (35, 14)\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 5.1 — DataFrames por split\n",
    "# ================================\n",
    "\n",
    "def build_split_df(split_name):\n",
    "    # Leer IDs del split\n",
    "    with open(DATASET_ROOT / \"splits\" / f\"{split_name}.txt\") as f:\n",
    "        ids = [l.strip() for l in f if l.strip()]\n",
    "\n",
    "    # Normalizar IDs\n",
    "    ids_norm = [split_path_to_index_id(x) for x in ids]\n",
    "\n",
    "    # Filtrar índice\n",
    "    df = index_df[index_df[\"video_id\"].isin(ids_norm)].reset_index(drop=True)\n",
    "\n",
    "    # Inferir label binario\n",
    "    df[\"label\"] = df[\"video_id\"].apply(infer_label_from_video_id)\n",
    "\n",
    "    print(f\"{split_name}: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "train_df = build_split_df(\"train\")\n",
    "val_df   = build_split_df(\"val\")\n",
    "test_df  = build_split_df(\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26577c8",
   "metadata": {},
   "source": [
    "## 5.2 Organización de salida de embeddings\n",
    "\n",
    "Los embeddings se almacenan en disco para evitar reprocesar frames\n",
    "en experimentos posteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cee75d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature root: /home/diego/Escritorio/Pruebas/tesispython/features/siglip_baseline\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 5.2 — Directorios de salida\n",
    "# ================================\n",
    "FEATURE_ROOT = NOTEBOOK_DIR / \"features\" / \"siglip_baseline\"\n",
    "FEATURE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    (FEATURE_ROOT / split).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Feature root:\", FEATURE_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e02529",
   "metadata": {},
   "source": [
    "## 5.3 Extracción y guardado de embeddings\n",
    "\n",
    "Para cada video:\n",
    "1. Se cargan los frames (clip).\n",
    "2. Se obtiene el embedding de video con SigLIP.\n",
    "3. Se guarda un archivo `.npy` por video.\n",
    "4. Se registra `video_id` y `label` en un CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2d9616b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Parte 5.3 — Extracción de embeddings\n",
    "# ================================\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_and_save_embeddings(df, split_name, encoder, processor):\n",
    "    rows = []\n",
    "\n",
    "    dataset = FrameClipDataset(\n",
    "        df,\n",
    "        PROCESSED_DIR,\n",
    "        split=split_name,\n",
    "        n_frames=N_FRAMES,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    out_dir = FEATURE_ROOT / split_name\n",
    "\n",
    "    for i in tqdm(range(len(dataset)), desc=f\"Extract {split_name}\"):\n",
    "        sample = dataset[i]\n",
    "        video_id = sample[\"video_id\"]\n",
    "        label = sample[\"label\"]\n",
    "        clip = sample[\"clip\"]\n",
    "\n",
    "        emb = encode_clip_siglip(encoder, processor, clip)  # [1, D]\n",
    "        emb = emb.squeeze(0).cpu().numpy()                  # [D]\n",
    "\n",
    "        np.save(out_dir / f\"{video_id}.npy\", emb)\n",
    "\n",
    "        rows.append({\n",
    "            \"video_id\": video_id,\n",
    "            \"label\": label,\n",
    "            \"feature_path\": str(out_dir / f\"{video_id}.npy\")\n",
    "        })\n",
    "\n",
    "    # Guardar índice de features\n",
    "    pd.DataFrame(rows).to_csv(out_dir / \"labels.csv\", index=False)\n",
    "    print(f\"Guardado {split_name}: {len(rows)} embeddings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41a6b9",
   "metadata": {},
   "source": [
    "## 5.4 Ejecución de la extracción (Baseline)\n",
    "\n",
    "Se extraen embeddings para los conjuntos de entrenamiento,\n",
    "validación y prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ccab96b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract train:  20%|██        | 35/175 [1:02:48<4:11:15, 107.68s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Parte 5.4 — Run extracción baseline\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ================================\u001b[39;00m\n\u001b[1;32m      4\u001b[0m encoder\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# asegurarse que está congelado\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mextract_and_save_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m extract_and_save_embeddings(val_df,   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m,   encoder, processor)\n\u001b[1;32m      8\u001b[0m extract_and_save_embeddings(test_df,  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m,  encoder, processor)\n",
      "Cell \u001b[0;32mIn[69], line 26\u001b[0m, in \u001b[0;36mextract_and_save_embeddings\u001b[0;34m(df, split_name, encoder, processor)\u001b[0m\n\u001b[1;32m     23\u001b[0m label \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     24\u001b[0m clip \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 26\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[43mencode_clip_siglip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [1, D]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m emb \u001b[38;5;241m=\u001b[39m emb\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()                  \u001b[38;5;66;03m# [D]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m np\u001b[38;5;241m.\u001b[39msave(out_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, emb)\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[63], line 17\u001b[0m, in \u001b[0;36mencode_clip_siglip\u001b[0;34m(encoder, processor, clip)\u001b[0m\n\u001b[1;32m     14\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimgs, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [T, D]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Pooling temporal\u001b[39;00m\n\u001b[1;32m     20\u001b[0m video_emb \u001b[38;5;241m=\u001b[39m feats\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)   \u001b[38;5;66;03m# [1, D]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/transformers/utils/generic.py:809\u001b[0m, in \u001b[0;36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m         cls_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    802\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    803\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    804\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    805\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    806\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    807\u001b[0m     )\n\u001b[0;32m--> 809\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalid_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/transformers/models/siglip/modeling_siglip.py:899\u001b[0m, in \u001b[0;36mSiglipModel.get_image_features\u001b[0;34m(self, pixel_values, interpolate_pos_encoding, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;129m@filter_out_non_signature_kwargs\u001b[39m()\n\u001b[1;32m    869\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_image_features\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    875\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[1;32m    876\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;124;03m        image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The image embeddings obtained by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m    ...     image_features = model.get_image_features(**inputs)\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 899\u001b[0m     vision_outputs: BaseModelOutputWithPooling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    904\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m vision_outputs\u001b[38;5;241m.\u001b[39mpooler_output\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pooled_output\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/transformers/models/siglip/modeling_siglip.py:700\u001b[0m, in \u001b[0;36mSiglipVisionTransformer.forward\u001b[0;34m(self, pixel_values, interpolate_pos_encoding, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    697\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseModelOutputWithPooling:\n\u001b[1;32m    698\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding)\n\u001b[0;32m--> 700\u001b[0m     encoder_outputs: BaseModelOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    706\u001b[0m     last_hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_layernorm(last_hidden_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/transformers/models/siglip/modeling_siglip.py:559\u001b[0m, in \u001b[0;36mSiglipEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m encoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 559\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutput(last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/transformers/models/siglip/modeling_siglip.py:458\u001b[0m, in \u001b[0;36mSiglipEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    457\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm2(hidden_states)\n\u001b[0;32m--> 458\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/transformers/models/siglip/modeling_siglip.py:424\u001b[0m, in \u001b[0;36mSiglipMLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 424\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(hidden_states)\n\u001b[1;32m    426\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/torch/nn/modules/linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 5.4 — Run extracción baseline\n",
    "# ================================\n",
    "encoder.eval()  # asegurarse que está congelado\n",
    "\n",
    "extract_and_save_embeddings(train_df, \"train\", encoder, processor)\n",
    "extract_and_save_embeddings(val_df,   \"val\",   encoder, processor)\n",
    "extract_and_save_embeddings(test_df,  \"test\",  encoder, processor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b3338",
   "metadata": {},
   "source": [
    "## 5.5 Verificación de embeddings guardados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "79379269",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/diego/Escritorio/Pruebas/tesispython/features/siglip_baseline/train/labels.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Parte 5.5 — Check rápido\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ================================\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m sample_row \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFEATURE_ROOT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m emb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(sample_row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_path\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, emb\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/vad310/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/diego/Escritorio/Pruebas/tesispython/features/siglip_baseline/train/labels.csv'"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 5.5 — Check rápido\n",
    "# ================================\n",
    "sample_row = pd.read_csv(FEATURE_ROOT / \"train\" / \"labels.csv\").iloc[0]\n",
    "emb = np.load(sample_row[\"feature_path\"])\n",
    "\n",
    "print(\"Embedding shape:\", emb.shape)\n",
    "print(\"Label:\", sample_row[\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d54157f",
   "metadata": {},
   "source": [
    "# Parte 5B — Extracción rápida (Subset)\n",
    "\n",
    "Objetivo:\n",
    "- Validar extremo a extremo que el pipeline funciona (frames → SigLIP → .npy + labels.csv)\n",
    "- Evitar tiempos largos ejecutando solo un subconjunto (p. ej., 10–20 videos)\n",
    "- Dejar el sistema listo para correr el conjunto completo en otro momento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738b18a1",
   "metadata": {},
   "source": [
    "## 5B.1 Subset de entrenamiento y directorio de salida\n",
    "\n",
    "Se seleccionan N videos del train para validar la extracción.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b445f6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG_N: 10\n",
      "Salida: /home/diego/Escritorio/Pruebas/tesispython/features/siglip_baseline_debug/train_debug\n",
      "train_df_debug shape: (10, 14)\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 5B.1 — Subset + Output dir\n",
    "# ================================\n",
    "DEBUG_N = 10   # súbelo a 20 si quieres, pero 10 es suficiente para validar\n",
    "\n",
    "train_df_debug = train_df.iloc[:DEBUG_N].copy()\n",
    "\n",
    "FEATURE_DEBUG_ROOT = NOTEBOOK_DIR / \"features\" / \"siglip_baseline_debug\"\n",
    "(FEATURE_DEBUG_ROOT / \"train_debug\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DEBUG_N:\", DEBUG_N)\n",
    "print(\"Salida:\", FEATURE_DEBUG_ROOT / \"train_debug\")\n",
    "print(\"train_df_debug shape:\", train_df_debug.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193bca24",
   "metadata": {},
   "source": [
    "## 5B.2 Extracción de embeddings para el subset\n",
    "\n",
    "Se guarda un `.npy` por video y un `labels.csv` para el split debug.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b5a1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract train_debug: 100%|██████████| 10/10 [19:37<00:00, 117.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado train_debug: 10 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 5B.2 — Extracción subset\n",
    "# ================================\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_and_save_embeddings_debug(df, split_name, encoder, processor, out_root):\n",
    "    rows = []\n",
    "\n",
    "    dataset = FrameClipDataset(\n",
    "        df,\n",
    "        PROCESSED_DIR,\n",
    "        split=\"train\",           # usamos frames del split train\n",
    "        n_frames=N_FRAMES,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    out_dir = out_root / split_name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    encoder.eval()\n",
    "\n",
    "    for i in tqdm(range(len(dataset)), desc=f\"Extract {split_name}\"):\n",
    "        sample = dataset[i]\n",
    "        video_id = sample[\"video_id\"]\n",
    "        label = sample[\"label\"]\n",
    "        clip = sample[\"clip\"]\n",
    "\n",
    "        emb = encode_clip_siglip(encoder, processor, clip)  # [1, D]\n",
    "        emb = emb.squeeze(0).cpu().numpy()                  # [D]\n",
    "\n",
    "        np.save(out_dir / f\"{video_id}.npy\", emb)\n",
    "\n",
    "        rows.append({\n",
    "            \"video_id\": video_id,\n",
    "            \"label\": label,\n",
    "            \"feature_path\": str(out_dir / f\"{video_id}.npy\")\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(out_dir / \"labels.csv\", index=False)\n",
    "    print(f\"Guardado {split_name}: {len(rows)} embeddings\")\n",
    "    return out_dir\n",
    "\n",
    "debug_out_dir = extract_and_save_embeddings_debug(\n",
    "    train_df_debug,\n",
    "    split_name=\"train_debug\",\n",
    "    encoder=encoder,\n",
    "    processor=processor,\n",
    "    out_root=FEATURE_DEBUG_ROOT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8558c78",
   "metadata": {},
   "source": [
    "## 5B.3 Verificación de archivos generados\n",
    "\n",
    "Se valida que:\n",
    "- existe `labels.csv`\n",
    "- existen archivos `.npy`\n",
    "- el embedding tiene dimensión esperada (1152)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52822963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.csv exists: True - /home/diego/Escritorio/Pruebas/tesispython/features/siglip_baseline_debug/train_debug/labels.csv\n",
      "labels_df_debug shape: (10, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>label</th>\n",
       "      <th>feature_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Videos__Normal_Videos_event__Normal_Videos_597...</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/diego/Escritorio/Pruebas/tesispython/fea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Videos__Normal_Videos_event__Normal_Videos_603...</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/diego/Escritorio/Pruebas/tesispython/fea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Videos__Arson__Arson046_x264</td>\n",
       "      <td>1</td>\n",
       "      <td>/home/diego/Escritorio/Pruebas/tesispython/fea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            video_id  label  \\\n",
       "0  Videos__Normal_Videos_event__Normal_Videos_597...      0   \n",
       "1  Videos__Normal_Videos_event__Normal_Videos_603...      0   \n",
       "2                       Videos__Arson__Arson046_x264      1   \n",
       "\n",
       "                                        feature_path  \n",
       "0  /home/diego/Escritorio/Pruebas/tesispython/fea...  \n",
       "1  /home/diego/Escritorio/Pruebas/tesispython/fea...  \n",
       "2  /home/diego/Escritorio/Pruebas/tesispython/fea...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primer embedding shape: (1152,)\n",
      "Primer label: 0\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 5B.3 — Check outputs\n",
    "# ================================\n",
    "labels_path = debug_out_dir / \"labels.csv\"\n",
    "print(\"labels.csv exists:\", labels_path.exists(), \"-\", labels_path)\n",
    "\n",
    "labels_df_debug = pd.read_csv(labels_path)\n",
    "print(\"labels_df_debug shape:\", labels_df_debug.shape)\n",
    "display(labels_df_debug.head(3))\n",
    "\n",
    "first_path = labels_df_debug.iloc[0][\"feature_path\"]\n",
    "emb = np.load(first_path)\n",
    "print(\"Primer embedding shape:\", emb.shape)\n",
    "print(\"Primer label:\", labels_df_debug.iloc[0][\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc55b53",
   "metadata": {},
   "source": [
    "# Parte 6 — Clasificador MLP (modo debug)\n",
    "\n",
    "En esta etapa se entrena un clasificador MLP simple sobre los embeddings\n",
    "extraídos en la Parte 5B (subset), con el objetivo de:\n",
    "\n",
    "- Validar el pipeline de clasificación\n",
    "- Verificar métricas y pérdidas\n",
    "- Preparar el código para el entrenamiento completo posterior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f8a408",
   "metadata": {},
   "source": [
    "## 6.1 Carga de embeddings y etiquetas\n",
    "\n",
    "Se cargan los embeddings `.npy` y las etiquetas desde `labels.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffcc6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (10, 1152)\n",
      "y shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 6.1 — Cargar features\n",
    "# ================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DEBUG_FEATURE_DIR = FEATURE_DEBUG_ROOT / \"train_debug\"\n",
    "labels_df = pd.read_csv(DEBUG_FEATURE_DIR / \"labels.csv\")\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for _, row in labels_df.iterrows():\n",
    "    emb = np.load(row[\"feature_path\"])\n",
    "    X.append(emb)\n",
    "    y.append(row[\"label\"])\n",
    "\n",
    "X = np.stack(X)   # [N, 1152]\n",
    "y = np.array(y)   # [N]\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f983de9",
   "metadata": {},
   "source": [
    "## 6.2 Split entrenamiento / validación (debug)\n",
    "\n",
    "Dado el tamaño reducido, se utiliza un split simple solo para validar el flujo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245507c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (7, 1152) (7,)\n",
      "Val: (3, 1152) (3,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Val:\",   X_val.shape,   y_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d29e3",
   "metadata": {},
   "source": [
    "## 6.3 Definición del clasificador MLP\n",
    "\n",
    "Se utiliza un MLP simple con una capa oculta.\n",
    "Este modelo se entrenará únicamente sobre embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b381afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=1152, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 6.3 — MLP\n",
    "# ================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "model = MLPClassifier(input_dim=1152).to(DEVICE)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3391ad34",
   "metadata": {},
   "source": [
    "## 6.4 Preparación de entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d2c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Parte 6.4 — Setup entrenamiento\n",
    "# ================================\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32).to(DEVICE)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.float32).to(DEVICE)\n",
    "y_val_t = torch.tensor(y_val, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d189f4",
   "metadata": {},
   "source": [
    "## 6.5 Entrenamiento del MLP (debug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb0eb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | train loss: 0.6679 | val loss: 0.5481\n",
      "Epoch 05 | train loss: 0.1228 | val loss: 0.3042\n",
      "Epoch 10 | train loss: 0.0349 | val loss: 0.2026\n",
      "Epoch 15 | train loss: 0.0044 | val loss: 0.1781\n",
      "Epoch 19 | train loss: 0.0016 | val loss: 0.1676\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 6.5 — Train loop\n",
    "# ================================\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits = model(X_train_t)\n",
    "    loss = criterion(logits, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validación\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = model(X_val_t)\n",
    "        val_loss = criterion(val_logits, y_val_t)\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
    "        print(f\"Epoch {epoch:02d} | train loss: {loss.item():.4f} | val loss: {val_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a055b7",
   "metadata": {},
   "source": [
    "## 6.6 Métricas (debug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f97e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 6.6 — Métricas\n",
    "# ================================\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    probs = torch.sigmoid(model(X_val_t)).cpu().numpy()\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_val, preds)\n",
    "f1  = f1_score(y_val, preds, zero_division=0)\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"F1:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f3d97",
   "metadata": {},
   "source": [
    "# Parte 7 — LoRA entrenado y evaluación del codificador\n",
    "\n",
    "Objetivo:\n",
    "1. Entrenar LoRA sobre el codificador SigLIP usando un objetivo binario (normal vs anomalía).\n",
    "2. Extraer embeddings con el codificador **ya adaptado** (SigLIP + LoRA entrenado).\n",
    "3. Entrenar y evaluar un MLP sobre esos embeddings para cuantificar el impacto de LoRA.\n",
    "\n",
    "Nota: En esta versión (debug) se usa batch_size=1 para simplificar el procesamiento de frames.\n",
    "Luego se puede escalar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6399692b",
   "metadata": {},
   "source": [
    "## 7.1 Subsets debug para entrenamiento de LoRA\n",
    "\n",
    "Se usa un subconjunto pequeño para validar el pipeline de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1941100b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df_debug: (10, 14) {1: 7, 0: 3}\n",
      "val_df_debug: (10, 14) {1: 8, 0: 2}\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 7.1 — Debug subsets\n",
    "# ================================\n",
    "DEBUG_TRAIN_N = 10\n",
    "DEBUG_VAL_N = 10\n",
    "\n",
    "train_df_debug = train_df.iloc[:DEBUG_TRAIN_N].copy()\n",
    "val_df_debug   = val_df.iloc[:DEBUG_VAL_N].copy()\n",
    "\n",
    "print(\"train_df_debug:\", train_df_debug.shape, train_df_debug[\"label\"].value_counts().to_dict())\n",
    "print(\"val_df_debug:\",   val_df_debug.shape,   val_df_debug[\"label\"].value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846372e6",
   "metadata": {},
   "source": [
    "## 7.2 DataLoaders (batch_size=1)\n",
    "\n",
    "Para simplificar el uso de `processor` (HuggingFace) sobre listas de imágenes,\n",
    "en debug usamos `batch_size=1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ad115a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batches: 10 val batches: 10\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 7.2 — Loaders (batch_size=1)\n",
    "# ================================\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds_lora = FrameClipDataset(\n",
    "    train_df_debug, PROCESSED_DIR, split=\"train\",\n",
    "    n_frames=N_FRAMES, transform=transform\n",
    ")\n",
    "val_ds_lora = FrameClipDataset(\n",
    "    val_df_debug, PROCESSED_DIR, split=\"val\",\n",
    "    n_frames=N_FRAMES, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds_lora, batch_size=1, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds_lora, batch_size=1, shuffle=False)\n",
    "\n",
    "print(\"train batches:\", len(train_loader), \"val batches:\", len(val_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cb2e35",
   "metadata": {},
   "source": [
    "## 7.3 Modelo de entrenamiento: SigLIP + LoRA + cabeza de clasificación\n",
    "\n",
    "Se añade una cabeza lineal binaria sobre el embedding promedio del clip.\n",
    "Se entrenan únicamente:\n",
    "- parámetros LoRA\n",
    "- parámetros de la cabeza (clasificador)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fe7037d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo LoRA+Head listo\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 7.3 — Wrapper con head\n",
    "# ================================\n",
    "import torch.nn as nn\n",
    "\n",
    "# Asegurar: encoder_lora ya existe (Parte 4.8) y peft está instalado.\n",
    "# Si no, vuelve a ejecutar Parte 4.7–4.9.\n",
    "\n",
    "class SiglipLoraWithHead(nn.Module):\n",
    "    def __init__(self, encoder_lora, processor, emb_dim=1152):\n",
    "        super().__init__()\n",
    "        self.encoder_lora = encoder_lora\n",
    "        self.processor = processor\n",
    "        self.head = nn.Linear(emb_dim, 1)\n",
    "\n",
    "    def forward(self, clip_tensor):\n",
    "        \"\"\"\n",
    "        clip_tensor: torch.Tensor [T, C, H, W] (batch_size=1 afuera)\n",
    "        output: logits [1]\n",
    "        \"\"\"\n",
    "        # clip_tensor viene sin batch; convertir frames a PIL\n",
    "        imgs = [transforms.ToPILImage()(clip_tensor[t].cpu()) for t in range(clip_tensor.shape[0])]\n",
    "        inputs = self.processor(images=imgs, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "        feats = self.encoder_lora.get_image_features(**inputs)  # [T, D]\n",
    "        video_emb = feats.mean(dim=0, keepdim=True)            # [1, D]\n",
    "        logits = self.head(video_emb).squeeze(1)               # [1]\n",
    "        return logits, video_emb\n",
    "\n",
    "model_lora_cls = SiglipLoraWithHead(encoder_lora, processor, emb_dim=1152).to(DEVICE)\n",
    "print(\"Modelo LoRA+Head listo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0d912",
   "metadata": {},
   "source": [
    "## 7.4 Configuración de entrenamiento (LoRA + head)\n",
    "\n",
    "Se entrena con BCEWithLogitsLoss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2110af0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N params trainables: 1991809\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 7.4 — Optimizer & loss\n",
    "# ================================\n",
    "import torch\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Solo parámetros entrenables (LoRA + head)\n",
    "params = [p for p in model_lora_cls.parameters() if p.requires_grad]\n",
    "print(\"N params trainables:\", sum(p.numel() for p in params))\n",
    "\n",
    "optimizer = torch.optim.AdamW(params, lr=1e-4, weight_decay=1e-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852d8bba",
   "metadata": {},
   "source": [
    "## 7.5 Entrenamiento LoRA (debug)\n",
    "\n",
    "Se entrena pocas épocas solo para validar flujo y obtener un LoRA no-trivial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d03412c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_lora_cls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m EPOCHS_LORA \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# debug; luego subes (ej. 5–10)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel_lora_cls\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS_LORA):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     model_lora_cls\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_lora_cls' is not defined"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Parte 7.5 — Train loop LoRA\n",
    "# ================================\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "EPOCHS_LORA = 3  # debug; luego subes (ej. 5–10)\n",
    "model_lora_cls.train()\n",
    "\n",
    "for epoch in range(EPOCHS_LORA):\n",
    "    # Train\n",
    "    model_lora_cls.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        clip = batch[\"clip\"].squeeze(0).to(DEVICE)   # [T,C,H,W]\n",
    "        y = torch.tensor([batch[\"label\"][0]], dtype=torch.float32).to(DEVICE)  # [1]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model_lora_cls(clip)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    # Val\n",
    "    model_lora_cls.eval()\n",
    "    val_losses = []\n",
    "    probs_all, y_all = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            clip = batch[\"clip\"].squeeze(0).to(DEVICE)\n",
    "            y = float(batch[\"label\"][0])\n",
    "            logits, _ = model_lora_cls(clip)\n",
    "            loss = criterion(logits, torch.tensor([y], dtype=torch.float32).to(DEVICE))\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "            probs_all.append(prob)\n",
    "            y_all.append(int(y))\n",
    "\n",
    "    preds_all = (np.array(probs_all) > 0.5).astype(int)\n",
    "    acc = accuracy_score(y_all, preds_all)\n",
    "    f1  = f1_score(y_all, preds_all, zero_division=0)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{EPOCHS_LORA} | \"\n",
    "        f\"train loss: {np.mean(train_losses):.4f} | \"\n",
    "        f\"val loss: {np.mean(val_losses):.4f} | \"\n",
    "        f\"val acc: {acc:.3f} | val f1: {f1:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56362758",
   "metadata": {},
   "source": [
    "## 7.6 Guardado del adaptador LoRA\n",
    "\n",
    "Se guarda el adaptador para reutilizarlo sin reentrenar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50302c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Parte 7.6 — Save LoRA adapter\n",
    "# ================================\n",
    "LORA_OUT = NOTEBOOK_DIR / \"checkpoints\" / \"siglip_lora_debug\"\n",
    "LORA_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Guardar solo el adaptador LoRA\n",
    "encoder_lora.save_pretrained(LORA_OUT)\n",
    "\n",
    "# Guardar head también (para referencia)\n",
    "torch.save(model_lora_cls.head.state_dict(), LORA_OUT / \"head.pt\")\n",
    "\n",
    "print(\"LoRA guardado en:\", LORA_OUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d25be",
   "metadata": {},
   "source": [
    "## 7.7 Extracción de embeddings con LoRA entrenado (debug)\n",
    "\n",
    "Se repite el procedimiento de extracción, pero usando `encoder_lora` entrenado.\n",
    "Esto permite evaluar el codificador con LoRA de forma comparable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd12016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Parte 7.7 — Extract embeddings (LoRA trained) debug\n",
    "# ================================\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "FEATURE_LORA_DEBUG_ROOT = NOTEBOOK_DIR / \"features\" / \"siglip_lora_debug\"\n",
    "(FEATURE_LORA_DEBUG_ROOT / \"train_debug\").mkdir(parents=True, exist_ok=True)\n",
    "(FEATURE_LORA_DEBUG_ROOT / \"val_debug\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def extract_embeddings_to_dir(df, split_name, split_frames_dirname, encoder_used, processor_used, out_root):\n",
    "    rows = []\n",
    "    dataset = FrameClipDataset(\n",
    "        df, PROCESSED_DIR, split=split_frames_dirname,\n",
    "        n_frames=N_FRAMES, transform=transform\n",
    "    )\n",
    "    out_dir = out_root / split_name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    encoder_used.eval()\n",
    "\n",
    "    for i in tqdm(range(len(dataset)), desc=f\"Extract {split_name}\"):\n",
    "        s = dataset[i]\n",
    "        video_id, label, clip = s[\"video_id\"], s[\"label\"], s[\"clip\"]\n",
    "        emb = encode_clip_siglip(encoder_used, processor_used, clip).squeeze(0).cpu().numpy()\n",
    "        np.save(out_dir / f\"{video_id}.npy\", emb)\n",
    "        rows.append({\"video_id\": video_id, \"label\": label, \"feature_path\": str(out_dir / f\"{video_id}.npy\")})\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(out_dir / \"labels.csv\", index=False)\n",
    "    print(f\"{split_name}: guardados {len(rows)} embeddings\")\n",
    "    return out_dir\n",
    "\n",
    "train_lora_dir = extract_embeddings_to_dir(\n",
    "    train_df_debug, split_name=\"train_debug\", split_frames_dirname=\"train\",\n",
    "    encoder_used=encoder_lora, processor_used=processor,\n",
    "    out_root=FEATURE_LORA_DEBUG_ROOT\n",
    ")\n",
    "\n",
    "val_lora_dir = extract_embeddings_to_dir(\n",
    "    val_df_debug, split_name=\"val_debug\", split_frames_dirname=\"val\",\n",
    "    encoder_used=encoder_lora, processor_used=processor,\n",
    "    out_root=FEATURE_LORA_DEBUG_ROOT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6b8748",
   "metadata": {},
   "source": [
    "## 7.8 Clasificación con MLP usando embeddings LoRA (debug)\n",
    "\n",
    "Se repite el MLP, pero entrenando sobre embeddings del codificador adaptado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e52c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Parte 7.8 — MLP sobre embeddings LoRA\n",
    "# ================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Cargar train embeddings LoRA debug\n",
    "train_labels = pd.read_csv(train_lora_dir / \"labels.csv\")\n",
    "X, y = [], []\n",
    "for _, row in train_labels.iterrows():\n",
    "    X.append(np.load(row[\"feature_path\"]))\n",
    "    y.append(int(row[\"label\"]))\n",
    "X = np.stack(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split train/val (debug)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# MLP igual que antes\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "mlp = MLPClassifier(1152).to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opt = torch.optim.Adam(mlp.parameters(), lr=1e-3)\n",
    "\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32).to(DEVICE)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).to(DEVICE)\n",
    "X_val_t   = torch.tensor(X_val, dtype=torch.float32).to(DEVICE)\n",
    "y_val_t   = torch.tensor(y_val, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# Entrenar rápido\n",
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    mlp.train()\n",
    "    opt.zero_grad()\n",
    "    logits = mlp(X_train_t)\n",
    "    loss = criterion(logits, y_train_t)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
    "        mlp.eval()\n",
    "        with torch.no_grad():\n",
    "            v_logits = mlp(X_val_t)\n",
    "            v_loss = criterion(v_logits, y_val_t)\n",
    "        print(f\"Epoch {epoch:02d} | train loss {loss.item():.4f} | val loss {v_loss.item():.4f}\")\n",
    "\n",
    "# Métricas\n",
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    probs = torch.sigmoid(mlp(X_val_t)).cpu().numpy()\n",
    "preds = (probs > 0.5).astype(int)\n",
    "acc = accuracy_score(y_val, preds)\n",
    "f1  = f1_score(y_val, preds, zero_division=0)\n",
    "print(\"LoRA-embeddings MLP | Accuracy:\", acc, \"| F1:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a7e1f",
   "metadata": {},
   "source": [
    "# Parte 8 — Métricas SigLIP (debug): Baseline vs LoRA\n",
    "\n",
    "En esta sección calculamos métricas estándar de clasificación para el caso SigLIP:\n",
    "- Accuracy, F1, Precision, Recall\n",
    "- ROC-AUC y PR-AUC (si hay ambas clases)\n",
    "- Matriz de confusión\n",
    "Y generamos un gráfico comparativo Baseline vs LoRA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee4de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Parte 8.1 — Cargar embeddings SigLIP (debug)\n",
    "# ================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_debug_split(feature_dir):\n",
    "    feature_dir = Path(feature_dir)\n",
    "    labels = pd.read_csv(feature_dir / \"labels.csv\")\n",
    "    X, y = [], []\n",
    "    for _, row in labels.iterrows():\n",
    "        X.append(np.load(row[\"feature_path\"]))\n",
    "        y.append(int(row[\"label\"]))\n",
    "    X = np.stack(X)  # [N, 1152]\n",
    "    y = np.array(y)  # [N]\n",
    "    return X, y\n",
    "\n",
    "BASELINE_DIR = NOTEBOOK_DIR / \"features\" / \"siglip_baseline_debug\" / \"train_debug\"\n",
    "LORA_DIR     = NOTEBOOK_DIR / \"features\" / \"siglip_lora_debug\"     / \"train_debug\"\n",
    "\n",
    "Xb, yb = load_debug_split(BASELINE_DIR)\n",
    "print(\"Baseline:\", Xb.shape, yb.shape, \"labels:\", np.unique(yb, return_counts=True))\n",
    "\n",
    "Xl, yl = load_debug_split(LORA_DIR)\n",
    "print(\"LoRA:\", Xl.shape, yl.shape, \"labels:\", np.unique(yl, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91965e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Parte 8.2 — Métricas SigLIP (debug)\n",
    "# ================================\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix\n",
    ")\n",
    "\n",
    "def predict_probs(mlp_model, X_np):\n",
    "    mlp_model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_t = torch.tensor(X_np, dtype=torch.float32).to(DEVICE)\n",
    "        probs = torch.sigmoid(mlp_model(X_t)).cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "def compute_metrics(y_true, probs, thr=0.5):\n",
    "    y_pred = (probs >= thr).astype(int)\n",
    "    out = {\n",
    "        \"accuracy\":  accuracy_score(y_true, y_pred),\n",
    "        \"f1\":        f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\":    recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"cm\":        confusion_matrix(y_true, y_pred),\n",
    "    }\n",
    "    # AUCs solo si hay 2 clases presentes\n",
    "    if len(np.unique(y_true)) == 2:\n",
    "        out[\"roc_auc\"] = roc_auc_score(y_true, probs)\n",
    "        out[\"pr_auc\"]  = average_precision_score(y_true, probs)\n",
    "    else:\n",
    "        out[\"roc_auc\"] = np.nan\n",
    "        out[\"pr_auc\"]  = np.nan\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f83b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Parte 8.3 — Resultados Baseline vs LoRA\n",
    "# ================================\n",
    "probs_b = predict_probs(model, Xb)   # model = MLP baseline (Parte 6)\n",
    "m_b = compute_metrics(yb, probs_b, thr=0.5)\n",
    "\n",
    "probs_l = predict_probs(mlp, Xl)     # mlp = MLP LoRA (Parte 7.8)\n",
    "m_l = compute_metrics(yl, probs_l, thr=0.5)\n",
    "\n",
    "print(\"=== Baseline (SigLIP) ===\")\n",
    "for k in [\"accuracy\",\"f1\",\"precision\",\"recall\",\"roc_auc\",\"pr_auc\"]:\n",
    "    print(k, \":\", m_b[k])\n",
    "print(\"Confusion matrix:\\n\", m_b[\"cm\"])\n",
    "\n",
    "print(\"\\n=== LoRA (SigLIP) ===\")\n",
    "for k in [\"accuracy\",\"f1\",\"precision\",\"recall\",\"roc_auc\",\"pr_auc\"]:\n",
    "    print(k, \":\", m_l[k])\n",
    "print(\"Confusion matrix:\\n\", m_l[\"cm\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965e2225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Parte 8.4 — Gráfico comparativo\n",
    "# ================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\"variant\":\"baseline\", \"accuracy\":m_b[\"accuracy\"], \"f1\":m_b[\"f1\"], \"precision\":m_b[\"precision\"], \"recall\":m_b[\"recall\"], \"roc_auc\":m_b[\"roc_auc\"], \"pr_auc\":m_b[\"pr_auc\"]},\n",
    "    {\"variant\":\"lora\",     \"accuracy\":m_l[\"accuracy\"], \"f1\":m_l[\"f1\"], \"precision\":m_l[\"precision\"], \"recall\":m_l[\"recall\"], \"roc_auc\":m_l[\"roc_auc\"], \"pr_auc\":m_l[\"pr_auc\"]},\n",
    "])\n",
    "\n",
    "ax = df.set_index(\"variant\")[[\"f1\",\"roc_auc\",\"pr_auc\",\"accuracy\",\"precision\",\"recall\"]].plot(kind=\"bar\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.title(\"SigLIP (debug) — Baseline vs LoRA\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
