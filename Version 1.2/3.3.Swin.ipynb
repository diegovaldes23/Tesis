{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15323c0e-ae3e-4c85-92c3-a95b7a5c6726",
   "metadata": {},
   "source": [
    "### Extracción de características a nivel de clip (UCF-Crime)\n",
    "\n",
    "Este notebook utiliza como entrada el archivo `processed/index_clips.csv`\n",
    "\n",
    "El archivo `index_clips.csv` contiene, para cada clip temporal:\n",
    "- La ruta al video original.\n",
    "- El rango temporal del clip (`start_frame`, `end_frame`).\n",
    "- La partición correspondiente (`train`, `val`, `test`).\n",
    "- La etiqueta binaria (normal vs anómalo) y la categoría asociada.\n",
    "- Los parámetros de segmentación utilizados (longitud del clip y solapamiento).\n",
    "\n",
    "A partir de este índice, se cargan los frames correspondientes a cada clip y se transforman en la\n",
    "representación requerida por los modelos evaluados, sin redefinir ni modificar la composición del\n",
    "conjunto experimental.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95097f3b-37a1-459c-a65d-f2e745d75125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo cargado correctamente\n",
      "Número total de clips: 145356\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>y</th>\n",
       "      <th>category</th>\n",
       "      <th>path</th>\n",
       "      <th>clip_idx</th>\n",
       "      <th>start_frame</th>\n",
       "      <th>end_frame</th>\n",
       "      <th>clip_len</th>\n",
       "      <th>stride</th>\n",
       "      <th>fps</th>\n",
       "      <th>n_frames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>/home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>/home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>48</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>/home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>/home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>80</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>/home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>96</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split  y category                                               path  \\\n",
       "0  train  0   Normal  /home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...   \n",
       "1  train  0   Normal  /home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...   \n",
       "2  train  0   Normal  /home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...   \n",
       "3  train  0   Normal  /home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...   \n",
       "4  train  0   Normal  /home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...   \n",
       "\n",
       "   clip_idx  start_frame  end_frame  clip_len  stride   fps  n_frames  \n",
       "0         0            0         32        32      16  30.0      2016  \n",
       "1         1           16         48        32      16  30.0      2016  \n",
       "2         2           32         64        32      16  30.0      2016  \n",
       "3         3           48         80        32      16  30.0      2016  \n",
       "4         4           64         96        32      16  30.0      2016  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Ruta al índice de clips\n",
    "INDEX_CLIPS_PATH = Path(\"processed/index_clips.csv\")\n",
    "\n",
    "# Verificar existencia\n",
    "assert INDEX_CLIPS_PATH.exists(), f\"No se encuentra el archivo: {INDEX_CLIPS_PATH}\"\n",
    "\n",
    "# Cargar CSV\n",
    "df_clips = pd.read_csv(INDEX_CLIPS_PATH)\n",
    "\n",
    "print(\"Archivo cargado correctamente\")\n",
    "print(\"Número total de clips:\", len(df_clips))\n",
    "\n",
    "display(df_clips.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5d46e76-697f-4df4-9bda-e87b15ccb54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clips por split:\n",
      "split\n",
      "train    106527\n",
      "val       19793\n",
      "test      19036\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Clips por clase (y):\n",
      "y\n",
      "0    73870\n",
      "1    71486\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Clips con rangos inválidos: 0\n",
      "Clips con path inexistente: 0\n"
     ]
    }
   ],
   "source": [
    "# Distribución por split\n",
    "print(\"Clips por split:\")\n",
    "print(df_clips[\"split\"].value_counts())\n",
    "\n",
    "# Distribución por clase\n",
    "print(\"\\nClips por clase (y):\")\n",
    "print(df_clips[\"y\"].value_counts())\n",
    "\n",
    "# Verificar rangos temporales válidos\n",
    "invalid_ranges = df_clips[df_clips[\"end_frame\"] <= df_clips[\"start_frame\"]]\n",
    "print(\"\\nClips con rangos inválidos:\", len(invalid_ranges))\n",
    "\n",
    "# Verificar paths únicos y existencia\n",
    "missing_paths = df_clips[~df_clips[\"path\"].apply(lambda p: Path(p).exists())]\n",
    "print(\"Clips con path inexistente:\", len(missing_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0bb3684-eb80-4a09-af81-8f7da8f2cb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Videos con clips en más de un split: 0\n"
     ]
    }
   ],
   "source": [
    "# Verificar que un mismo video no aparezca en más de un split\n",
    "video_split_counts = df_clips.groupby(\"path\")[\"split\"].nunique()\n",
    "n_leak = int((video_split_counts > 1).sum())\n",
    "\n",
    "print(\"Videos con clips en más de un split:\", n_leak)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de18d744-8cc9-4718-bb64-83a298aa47c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parámetros del input\n",
    "T = 8                \n",
    "IMG_SIZE = 224       \n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "def uniform_sample_indices(start_f: int, end_f: int, T: int):\n",
    "    n = max(1, end_f - start_f)\n",
    "    idx = np.linspace(0, n - 1, T).round().astype(int)\n",
    "    return (start_f + idx).astype(int)\n",
    "\n",
    "class ClipDataset(Dataset):\n",
    "    def __init__(self, df, T=8, img_size=224):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.T = T\n",
    "        self.img_size = img_size\n",
    "        self.mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "        self.std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        path = row[\"path\"]\n",
    "        start_f = int(row[\"start_frame\"])\n",
    "        end_f   = int(row[\"end_frame\"])\n",
    "        y = int(row[\"y\"])\n",
    "\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        if not cap.isOpened():\n",
    "            raise RuntimeError(f\"No pude abrir video: {path}\")\n",
    "\n",
    "        frame_ids = uniform_sample_indices(start_f, end_f, self.T)\n",
    "\n",
    "        frames = []\n",
    "        last_good = None\n",
    "        for fid in frame_ids:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(fid))\n",
    "            ok, frame = cap.read()\n",
    "\n",
    "            if not ok:\n",
    "                if last_good is None:\n",
    "                    frame = np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8)\n",
    "                else:\n",
    "                    frame = last_good\n",
    "            else:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = cv2.resize(frame, (self.img_size, self.img_size), interpolation=cv2.INTER_LINEAR)\n",
    "                last_good = frame\n",
    "\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # (T,H,W,C) -> float [0,1]\n",
    "        arr = np.stack(frames).astype(np.float32) / 255.0\n",
    "        arr = (arr - self.mean) / self.std\n",
    "\n",
    "        # -> (C,T,H,W)\n",
    "        arr = np.transpose(arr, (3, 0, 1, 2))\n",
    "        clip = torch.from_numpy(arr)  # float32\n",
    "\n",
    "        return clip, torch.tensor(y, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9206d2bd-75bf-4de7-a9db-38caba109907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DIINF/dvaldes/venvs/tesis/lib/python3.10/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb shape: torch.Size([16, 3, 8, 224, 224])\n",
      "yb shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "df_train = df_clips[df_clips[\"split\"]==\"train\"].copy()\n",
    "df_val   = df_clips[df_clips[\"split\"]==\"val\"].copy()\n",
    "df_test  = df_clips[df_clips[\"split\"]==\"test\"].copy()\n",
    "\n",
    "train_ds = ClipDataset(df_train, T=T, img_size=IMG_SIZE)\n",
    "val_ds   = ClipDataset(df_val,   T=T, img_size=IMG_SIZE)\n",
    "test_ds = ClipDataset(df_test, T=T, img_size=IMG_SIZE)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"xb shape:\", xb.shape)  # esperado: (B, 3, 8, 224, 224)\n",
    "print(\"yb shape:\", yb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb555406-cccd-4bab-b01a-a1529c9fafae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CLASS: <class 'torchvision.models.video.swin_transformer.SwinTransformer3d'>\n",
      "MODEL MODULE: torchvision.models.video.swin_transformer\n",
      "Head: Identity()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.video import swin3d_t, Swin3D_T_Weights\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "WEIGHTS = Swin3D_T_Weights.DEFAULT\n",
    "encoder = swin3d_t(weights=WEIGHTS).to(DEVICE).eval()\n",
    "\n",
    "for p in encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Clave: reemplazar la cabeza de clasificación para obtener embeddings\n",
    "encoder.head = nn.Identity()\n",
    "\n",
    "print(\"MODEL CLASS:\", encoder.__class__)\n",
    "print(\"MODEL MODULE:\", encoder.__class__.__module__)\n",
    "print(\"Head:\", encoder.head)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0322d71-3f81-4132-8cc2-cebc09720524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb shape: torch.Size([16, 3, 8, 224, 224])\n",
      "emb shape: torch.Size([16, 768])\n",
      "D: 768\n"
     ]
    }
   ],
   "source": [
    "xb, yb = next(iter(train_loader))\n",
    "print(\"xb shape:\", xb.shape)  # esperado: (B, C, T, H, W)\n",
    "\n",
    "xb = xb.to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb = encoder(xb)\n",
    "\n",
    "print(\"emb shape:\", emb.shape)  # (B, D)\n",
    "D = int(emb.shape[1])\n",
    "print(\"D:\", D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a8c3444-1630-478b-9177-bd0796749370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def extract_embeddings(loader, encoder, X_mm, y_mm, desc=\"split\"):\n",
    "    encoder.eval()\n",
    "    offset = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(loader, desc=desc):\n",
    "            xb = xb.to(DEVICE)          # (B, C, T, H, W)\n",
    "            emb = encoder(xb)           # (B, D) porque head=Identity\n",
    "\n",
    "            emb_np = emb.detach().cpu().numpy().astype(X_mm.dtype, copy=False)\n",
    "            y_np   = yb.detach().cpu().numpy().astype(y_mm.dtype, copy=False)\n",
    "\n",
    "            bs = emb_np.shape[0]\n",
    "            X_mm[offset:offset+bs] = emb_np\n",
    "            y_mm[offset:offset+bs] = y_np\n",
    "            offset += bs\n",
    "\n",
    "    X_mm.flush(); y_mm.flush()\n",
    "    print(desc, \"done:\", offset)\n",
    "    return offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57e586e6-0d5a-4432-b5ee-9a645a89149f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memmaps creados:\n",
      "Train: (106527, 768) (106527,)\n",
      "Val:   (19793, 768) (19793,)\n",
      "Test:  (19036, 768) (19036,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "PROCESSED = Path(\"processed\")\n",
    "PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "n_train = len(train_loader.dataset)\n",
    "n_val   = len(val_loader.dataset)\n",
    "n_test  = len(test_loader.dataset)\n",
    "\n",
    "def create_memmap(path: Path, shape, dtype):\n",
    "    return np.memmap(str(path), mode=\"w+\", dtype=dtype, shape=shape)\n",
    "\n",
    "X_train = create_memmap(PROCESSED / \"emb_swin3d_t_train.mmap\", (n_train, D), \"float16\")\n",
    "y_train = create_memmap(PROCESSED / \"y_train.mmap\",            (n_train,),    \"int8\")\n",
    "\n",
    "X_val   = create_memmap(PROCESSED / \"emb_swin3d_t_val.mmap\",   (n_val, D),   \"float16\")\n",
    "y_val   = create_memmap(PROCESSED / \"y_val.mmap\",              (n_val,),     \"int8\")\n",
    "\n",
    "X_test  = create_memmap(PROCESSED / \"emb_swin3d_t_test.mmap\",  (n_test, D),  \"float16\")\n",
    "y_test  = create_memmap(PROCESSED / \"y_test.mmap\",             (n_test,),    \"int8\")\n",
    "\n",
    "print(\"Memmaps creados:\")\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Val:  \", X_val.shape, y_val.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a336b414-431f-40b2-bfb7-a0cf0db105f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6658/6658 [56:11<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done: 106527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1238/1238 [10:13<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val done: 19793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1190/1190 [09:56<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test done: 19036\n",
      "Written train: 106527 expected: 106527\n",
      "Written val:   19793 expected: 19793\n",
      "Written test:  19036 expected: 19036\n",
      "OK: extracción completa.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n1 = extract_embeddings(train_loader, encoder, X_train, y_train, desc=\"train\")\n",
    "n2 = extract_embeddings(val_loader,   encoder, X_val,   y_val,   desc=\"val\")\n",
    "n3 = extract_embeddings(test_loader,  encoder, X_test,  y_test,  desc=\"test\")\n",
    "\n",
    "print(\"Written train:\", n1, \"expected:\", n_train)\n",
    "print(\"Written val:  \", n2, \"expected:\", n_val)\n",
    "print(\"Written test: \", n3, \"expected:\", n_test)\n",
    "\n",
    "assert n1 == n_train and n2 == n_val and n3 == n_test, \"Mismatch escrito vs tamaño de dataset.\"\n",
    "print(\"OK: extracción completa.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38ce52f8-f0ea-4f5e-927e-c50519964af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: shape=(106527, 768) dtype=float16 min=-2.8359 max=2.8438 mean=-0.0018 std=inf\n",
      "X_val: shape=(19793, 768) dtype=float16 min=-2.5117 max=2.6406 mean=-0.0016 std=inf\n",
      "X_test: shape=(19036, 768) dtype=float16 min=-2.4766 max=2.5742 mean=-0.0019 std=inf\n",
      "y_train dist: {np.int8(0): np.int64(54009), np.int8(1): np.int64(52518)}\n",
      "y_val dist:   {np.int8(0): np.int64(10720), np.int8(1): np.int64(9073)}\n",
      "y_test dist:  {np.int8(0): np.int64(9141), np.int8(1): np.int64(9895)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def emb_stats(X_mm, name):\n",
    "    X = np.array(X_mm, copy=False)\n",
    "    print(f\"{name}: shape={X.shape} dtype={X.dtype} \"\n",
    "          f\"min={float(X.min()):.4f} max={float(X.max()):.4f} \"\n",
    "          f\"mean={float(X.mean()):.4f} std={float(X.std()):.4f}\")\n",
    "\n",
    "emb_stats(X_train, \"X_train\")\n",
    "emb_stats(X_val,   \"X_val\")\n",
    "emb_stats(X_test,  \"X_test\")\n",
    "\n",
    "print(\"y_train dist:\", dict(zip(*np.unique(y_train, return_counts=True))))\n",
    "print(\"y_val dist:  \", dict(zip(*np.unique(y_val,   return_counts=True))))\n",
    "print(\"y_test dist: \", dict(zip(*np.unique(y_test,  return_counts=True))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "794d275a-91be-4501-a87a-ffa9d1df8410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest guardado: processed/manifest_swin3d_t.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "manifest = {\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"encoder\": \"torchvision/swin3d_t\",\n",
    "    \"weights\": \"Swin3D_T_Weights.DEFAULT\",\n",
    "    \"T\": int(T),\n",
    "    \"img_size\": int(IMG_SIZE),\n",
    "    \"batch_size\": int(BATCH_SIZE),\n",
    "    \"embedding_dim\": int(D),\n",
    "    \"dtype\": \"float16\",\n",
    "    \"files\": {\n",
    "        \"X_train\": \"processed/emb_swin3d_t_train.mmap\",\n",
    "        \"y_train\": \"processed/y_train.mmap\",\n",
    "        \"X_val\":   \"processed/emb_swin3d_t_val.mmap\",\n",
    "        \"y_val\":   \"processed/y_val.mmap\",\n",
    "        \"X_test\":  \"processed/emb_swin3d_t_test.mmap\",\n",
    "        \"y_test\":  \"processed/y_test.mmap\",\n",
    "    }\n",
    "}\n",
    "\n",
    "out_manifest = PROCESSED / \"manifest_swin3d_t.json\"\n",
    "out_manifest.write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "print(\"Manifest guardado:\", out_manifest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa1a303-33be-4b96-bac5-ed94b3263915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tesis)",
   "language": "python",
   "name": "tesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
