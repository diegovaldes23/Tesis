{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15323c0e-ae3e-4c85-92c3-a95b7a5c6726",
   "metadata": {},
   "source": [
    "### Extracción de características a nivel de clip (UCF-Crime)\n",
    "\n",
    "Este notebook utiliza como entrada el archivo `processed/index_clips.csv`\n",
    "\n",
    "El archivo `index_clips.csv` contiene, para cada clip temporal:\n",
    "- La ruta al video original.\n",
    "- El rango temporal del clip (`start_frame`, `end_frame`).\n",
    "- La partición correspondiente (`train`, `val`, `test`).\n",
    "- La etiqueta binaria (normal vs anómalo) y la categoría asociada.\n",
    "- Los parámetros de segmentación utilizados (longitud del clip y solapamiento).\n",
    "\n",
    "A partir de este índice, se cargan los frames correspondientes a cada clip y se transforman en la\n",
    "representación requerida por los modelos evaluados, sin redefinir ni modificar la composición del\n",
    "conjunto experimental.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95097f3b-37a1-459c-a65d-f2e745d75125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo cargado correctamente\n",
      "Número total de clips: 145356\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>y</th>\n",
       "      <th>category</th>\n",
       "      <th>path</th>\n",
       "      <th>clip_idx</th>\n",
       "      <th>start_frame</th>\n",
       "      <th>end_frame</th>\n",
       "      <th>clip_len</th>\n",
       "      <th>stride</th>\n",
       "      <th>fps</th>\n",
       "      <th>n_frames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>/home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>/home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>48</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>/home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>/home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>80</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>/home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>96</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split  y category                                               path  \\\n",
       "0  train  0   Normal  /home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...   \n",
       "1  train  0   Normal  /home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...   \n",
       "2  train  0   Normal  /home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...   \n",
       "3  train  0   Normal  /home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...   \n",
       "4  train  0   Normal  /home/DIINF/dvaldes/tesis/UCF_Crime/Training-N...   \n",
       "\n",
       "   clip_idx  start_frame  end_frame  clip_len  stride   fps  n_frames  \n",
       "0         0            0         32        32      16  30.0      2016  \n",
       "1         1           16         48        32      16  30.0      2016  \n",
       "2         2           32         64        32      16  30.0      2016  \n",
       "3         3           48         80        32      16  30.0      2016  \n",
       "4         4           64         96        32      16  30.0      2016  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Ruta al índice de clips\n",
    "INDEX_CLIPS_PATH = Path(\"processed/index_clips.csv\")\n",
    "\n",
    "# Verificar existencia\n",
    "assert INDEX_CLIPS_PATH.exists(), f\"No se encuentra el archivo: {INDEX_CLIPS_PATH}\"\n",
    "\n",
    "# Cargar CSV\n",
    "df_clips = pd.read_csv(INDEX_CLIPS_PATH)\n",
    "\n",
    "print(\"Archivo cargado correctamente\")\n",
    "print(\"Número total de clips:\", len(df_clips))\n",
    "\n",
    "display(df_clips.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d46e76-697f-4df4-9bda-e87b15ccb54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clips por split:\n",
      "split\n",
      "train    106527\n",
      "val       19793\n",
      "test      19036\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Clips por clase (y):\n",
      "y\n",
      "0    73870\n",
      "1    71486\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Clips con rangos inválidos: 0\n",
      "Clips con path inexistente: 0\n"
     ]
    }
   ],
   "source": [
    "# Distribución por split\n",
    "print(\"Clips por split:\")\n",
    "print(df_clips[\"split\"].value_counts())\n",
    "\n",
    "# Distribución por clase\n",
    "print(\"\\nClips por clase (y):\")\n",
    "print(df_clips[\"y\"].value_counts())\n",
    "\n",
    "# Verificar rangos temporales válidos\n",
    "invalid_ranges = df_clips[df_clips[\"end_frame\"] <= df_clips[\"start_frame\"]]\n",
    "print(\"\\nClips con rangos inválidos:\", len(invalid_ranges))\n",
    "\n",
    "# Verificar paths únicos y existencia\n",
    "missing_paths = df_clips[~df_clips[\"path\"].apply(lambda p: Path(p).exists())]\n",
    "print(\"Clips con path inexistente:\", len(missing_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0bb3684-eb80-4a09-af81-8f7da8f2cb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Videos con clips en más de un split: 0\n"
     ]
    }
   ],
   "source": [
    "# Verificar que un mismo video no aparezca en más de un split\n",
    "video_split_counts = df_clips.groupby(\"path\")[\"split\"].nunique()\n",
    "n_leak = int((video_split_counts > 1).sum())\n",
    "\n",
    "print(\"Videos con clips en más de un split:\", n_leak)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939a4f44-3d6f-4c7e-a80c-9295a8a75bad",
   "metadata": {},
   "source": [
    "# 1. Pasar clips a tensor\n",
    "Construye un DataLoader de clips que, para cada fila del index_clips.csv, lee un rango de frames del video, muestra T frames, los normaliza y los entrega como tensor 5D listo para pasar por TimeSformer.\n",
    "\n",
    "Variables:\n",
    "\n",
    "- T = 8\n",
    "Número de frames que verá el encoder por clip. En TimeSformer (checkpoint típico) 8 es estándar.\n",
    "\n",
    "- IMG_SIZE = 224\n",
    "Tamaño espacial final por frame. 224×224 es el tamaño estándar de la mayoría de checkpoints preentrenados (evita problemas con embeddings posicionales).\n",
    "\n",
    "- BATCH_SIZE = 8\n",
    "Cuántos clips procesas en paralelo. Impacta memoria GPU y velocidad.\n",
    "\n",
    "- NUM_WORKERS = 4\n",
    "Paraleliza la lectura/decodificación de video en CPU (acelera el input pipeline).\n",
    "\n",
    "- mean/std (ImageNet)\n",
    "Normalización estándar para modelos preentrenados. Alinea la escala de píxeles con el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0595dee-afe4-4a5b-b866-c9004fc6d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parámetros del input\n",
    "T = 8                \n",
    "IMG_SIZE = 224       \n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "def uniform_sample_indices(start_f: int, end_f: int, T: int):\n",
    "    n = max(1, end_f - start_f)\n",
    "    idx = np.linspace(0, n - 1, T).round().astype(int)\n",
    "    return (start_f + idx).astype(int)\n",
    "\n",
    "class ClipDataset(Dataset):\n",
    "    def __init__(self, df, T=8, img_size=224):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.T = T\n",
    "        self.img_size = img_size\n",
    "        self.mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "        self.std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        path = row[\"path\"]\n",
    "        start_f = int(row[\"start_frame\"])\n",
    "        end_f   = int(row[\"end_frame\"])\n",
    "        y = int(row[\"y\"])\n",
    "\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        if not cap.isOpened():\n",
    "            raise RuntimeError(f\"No pude abrir video: {path}\")\n",
    "\n",
    "        frame_ids = uniform_sample_indices(start_f, end_f, self.T)\n",
    "\n",
    "        frames = []\n",
    "        last_good = None\n",
    "        for fid in frame_ids:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(fid))\n",
    "            ok, frame = cap.read()\n",
    "\n",
    "            if not ok:\n",
    "                if last_good is None:\n",
    "                    frame = np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8)\n",
    "                else:\n",
    "                    frame = last_good\n",
    "            else:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = cv2.resize(frame, (self.img_size, self.img_size), interpolation=cv2.INTER_LINEAR)\n",
    "                last_good = frame\n",
    "\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # (T,H,W,C) -> float [0,1]\n",
    "        arr = np.stack(frames).astype(np.float32) / 255.0\n",
    "        arr = (arr - self.mean) / self.std\n",
    "\n",
    "        # -> (C,T,H,W)\n",
    "        arr = np.transpose(arr, (3, 0, 1, 2))\n",
    "        clip = torch.from_numpy(arr)  # float32\n",
    "\n",
    "        return clip, torch.tensor(y, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd1539bd-8a5b-4a80-a6d5-1c0345e1c20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DIINF/dvaldes/venvs/tesis/lib/python3.10/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb shape: torch.Size([16, 3, 8, 224, 224])\n",
      "yb shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "df_train = df_clips[df_clips[\"split\"]==\"train\"].copy()\n",
    "df_val   = df_clips[df_clips[\"split\"]==\"val\"].copy()\n",
    "df_test  = df_clips[df_clips[\"split\"]==\"test\"].copy()\n",
    "\n",
    "train_ds = ClipDataset(df_train, T=T, img_size=IMG_SIZE)\n",
    "val_ds   = ClipDataset(df_val,   T=T, img_size=IMG_SIZE)\n",
    "test_ds = ClipDataset(df_test, T=T, img_size=IMG_SIZE)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"xb shape:\", xb.shape)  # esperado: (B, 3, 8, 224, 224)\n",
    "print(\"yb shape:\", yb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64352947-1ba7-4881-8142-6de79fe7d270",
   "metadata": {},
   "source": [
    "Significado de cada dimensión\n",
    "xb: [B, C, T, H, W]\n",
    "\n",
    "- B = 1 → Batch size (número de clips procesados en paralelo).\n",
    "\n",
    "- C = 3 → Canales de color (RGB).\n",
    "\n",
    "- T = 8 → Número de frames que ve TimeSformer por clip.\n",
    "\n",
    "- H = 224 → Alto de la imagen (resize estándar).\n",
    "\n",
    "- W = 224 → Ancho de la imagen (resize estándar).\n",
    "\n",
    "En resumen:\n",
    "Cada batch contiene 1 clips, cada clip tiene 8 frames RGB de 224×224."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83afee3d-8453-4f7d-9ccf-047d7847ee1b",
   "metadata": {},
   "source": [
    "# 2.Carga y congelamiento de TimeSformer\n",
    "\n",
    "Este bloque carga el modelo TimeSformer preentrenado, lo mueve a GPU (si está disponible) y congela sus parámetros para usarlo como feature extractor.\n",
    "\n",
    "Variables importantes\n",
    "\n",
    "- DEVICE: define si el modelo corre en \"cuda\" (GPU) o \"cpu\".\n",
    "\n",
    "- TIMESFORMER_CKPT: checkpoint utilizado (facebook/timesformer-base-finetuned-k400), entrenado en Kinetics-400.\n",
    "\n",
    "- image_processor: contiene configuración estándar de preprocesamiento (mean/std).\n",
    "\n",
    "- encoder: backbone que extrae representaciones espacio-temporales del clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2304eb6-faf8-43ec-a1c6-bb3aab822688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK loaded: facebook/timesformer-base-finetuned-k400\n",
      "processor mean/std: [0.45, 0.45, 0.45] [0.225, 0.225, 0.225]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import TimesformerModel, AutoImageProcessor\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "TIMESFORMER_CKPT = \"facebook/timesformer-base-finetuned-k400\"\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(TIMESFORMER_CKPT)\n",
    "encoder = TimesformerModel.from_pretrained(TIMESFORMER_CKPT)\n",
    "\n",
    "encoder = encoder.to(DEVICE)\n",
    "encoder.eval()\n",
    "for p in encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"OK loaded:\", TIMESFORMER_CKPT)\n",
    "print(\"processor mean/std:\", image_processor.image_mean, image_processor.image_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad2c74-d8f2-4227-bb98-60dac4b5ff59",
   "metadata": {},
   "source": [
    "Se cambia el orden de dimensiones porque TimeSformer espera los clips en formato (batch, frames, canales, alto, ancho) y nuestro tensor estaba en formato (batch, canales, frames, alto, ancho)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecba8164-bb31-423f-8e7c-ee28c76b0640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values shape: torch.Size([16, 8, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "xb = xb.to(DEVICE)\n",
    "\n",
    "# (B, C, T, H, W) -> (B, T, C, H, W)\n",
    "pixel_values = xb.permute(0, 2, 1, 3, 4).contiguous()\n",
    "print(\"pixel_values shape:\", pixel_values.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac22b7d3-12f8-4e9c-84c3-34c5686c13c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out keys: odict_keys(['last_hidden_state'])\n",
      "last_hidden_state shape: torch.Size([16, 1569, 768])\n",
      "CLS shape: torch.Size([16, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = encoder(pixel_values=pixel_values)\n",
    "\n",
    "print(\"out keys:\", out.keys())\n",
    "print(\"last_hidden_state shape:\", out.last_hidden_state.shape)\n",
    "\n",
    "# CLS token\n",
    "cls = out.last_hidden_state[:, 0, :]   # (B, D)\n",
    "print(\"CLS shape:\", cls.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d3c71f-a61a-44eb-b526-087bcbd1e994",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "147e85dc-c1e3-4c39-9b2f-e4c690723ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLS mean: -0.016693538054823875\n",
      "CLS std: 0.9742990732192993\n",
      "Any NaN: False\n"
     ]
    }
   ],
   "source": [
    "cls_cpu = cls.detach().cpu()\n",
    "print(\"CLS mean:\", float(cls_cpu.mean()))\n",
    "print(\"CLS std:\", float(cls_cpu.std()))\n",
    "print(\"Any NaN:\", torch.isnan(cls_cpu).any().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bfd13c-cd67-4c65-b721-873cb2dfc798",
   "metadata": {},
   "source": [
    "# 3 Extracción de emebdings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a640ae4-a3d1-4fff-ae90-91bb8381395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def create_memmap(path, shape, dtype=\"float16\"):\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    return np.memmap(path, mode=\"w+\", dtype=dtype, shape=shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a026e101-f4db-498f-91a3-9a30383df5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dim: 768\n"
     ]
    }
   ],
   "source": [
    "D = cls.shape[1]\n",
    "print(\"Embedding dim:\", D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b299f907-ab7b-416e-9ef6-9bc339d3ab3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106527 19793 19036\n"
     ]
    }
   ],
   "source": [
    "df_train = df_clips[df_clips[\"split\"]==\"train\"].copy()\n",
    "df_val   = df_clips[df_clips[\"split\"]==\"val\"].copy()\n",
    "df_test  = df_clips[df_clips[\"split\"]==\"test\"].copy()\n",
    "\n",
    "train_ds = ClipDataset(df_train, T=T, img_size=IMG_SIZE)\n",
    "val_ds   = ClipDataset(df_val,   T=T, img_size=IMG_SIZE)\n",
    "test_ds  = ClipDataset(df_test,  T=T, img_size=IMG_SIZE)\n",
    "\n",
    "print(len(train_ds), len(val_ds), len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9789118e-c8d4-4af3-ab81-43184a9fe3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "X_train = create_memmap(\"processed/emb_timesformer_cls_train.mmap\",\n",
    "                        (len(train_ds), D),\n",
    "                        dtype=\"float16\")\n",
    "y_train = create_memmap(\"processed/y_train.mmap\",\n",
    "                        (len(train_ds),),\n",
    "                        dtype=\"int8\")\n",
    "\n",
    "# Val\n",
    "X_val = create_memmap(\"processed/emb_timesformer_cls_val.mmap\",\n",
    "                      (len(val_ds), D),\n",
    "                      dtype=\"float16\")\n",
    "y_val = create_memmap(\"processed/y_val.mmap\",\n",
    "                      (len(val_ds),),\n",
    "                      dtype=\"int8\")\n",
    "\n",
    "# Test\n",
    "X_test = create_memmap(\"processed/emb_timesformer_cls_test.mmap\",\n",
    "                       (len(test_ds), D),\n",
    "                       dtype=\"float16\")\n",
    "y_test = create_memmap(\"processed/y_test.mmap\",\n",
    "                       (len(test_ds),),\n",
    "                       dtype=\"int8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb2a66f1-ac93-411d-b477-e1b6d73e6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(loader, encoder, X_mm, y_mm, split_name=\"train\"):\n",
    "    encoder.eval()\n",
    "\n",
    "    total_batches = len(loader)\n",
    "    print(f\"\\nExtracting {split_name.upper()} embeddings...\")\n",
    "    print(f\"Total batches: {total_batches}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        offset = 0\n",
    "\n",
    "        for xb, yb in tqdm(loader, desc=f\"{split_name}\", leave=True):\n",
    "\n",
    "            xb = xb.to(DEVICE)\n",
    "            pixel_values = xb.permute(0, 2, 1, 3, 4).contiguous()\n",
    "\n",
    "            out = encoder(pixel_values=pixel_values)\n",
    "            cls = out.last_hidden_state[:, 0, :]  # (B, D)\n",
    "\n",
    "            cls_np = cls.detach().cpu().numpy().astype(X_mm.dtype, copy=False)\n",
    "            y_np = yb.numpy().astype(y_mm.dtype, copy=False)\n",
    "\n",
    "            batch_size = cls_np.shape[0]\n",
    "\n",
    "            X_mm[offset:offset+batch_size] = cls_np\n",
    "            y_mm[offset:offset+batch_size] = y_np\n",
    "\n",
    "            offset += batch_size\n",
    "\n",
    "    X_mm.flush()\n",
    "    y_mm.flush()\n",
    "\n",
    "    print(f\"{split_name.upper()} extraction done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4671d21c-d1a8-4d72-991a-7d28e922b4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting TRAIN embeddings...\n",
      "Total batches: 6658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████████████████████████| 6658/6658 [1:16:24<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN extraction done.\n",
      "\n",
      "Extracting VAL embeddings...\n",
      "Total batches: 1238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████████████████████████████| 1238/1238 [14:15<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL extraction done.\n",
      "\n",
      "Extracting TEST embeddings...\n",
      "Total batches: 1190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|█████████████████████████████████| 1190/1190 [13:42<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST extraction done.\n",
      "\n",
      "All embeddings extracted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "extract_embeddings(train_loader, encoder, X_train, y_train, split_name=\"train\")\n",
    "extract_embeddings(val_loader, encoder, X_val, y_val, split_name=\"val\")\n",
    "extract_embeddings(test_loader, encoder, X_test, y_test, split_name=\"test\")\n",
    "\n",
    "print(\"\\nAll embeddings extracted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3273c9a-ba24-4671-af74-ee70558b310f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (106527, 768) (106527,)\n",
      "Val:   (19793, 768) (19793,)\n",
      "Test:  (19036, 768) (19036,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Val:  \", X_val.shape, y_val.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0aab756-590c-4e40-8192-cc6e39cc3a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[train]\n",
      "  finite: True\n",
      "  mean: -0.0163530632853508\n",
      "  std: 0.9735729098320007\n",
      "  min/max: -6.234375 5.6015625\n",
      "  y counts: {0: 2556, 1: 2444}\n",
      "\n",
      "[val]\n",
      "  finite: True\n",
      "  mean: -0.01572292298078537\n",
      "  std: 0.9724651575088501\n",
      "  min/max: -6.6953125 5.98828125\n",
      "  y counts: {0: 2762, 1: 2238}\n",
      "\n",
      "[test]\n",
      "  finite: True\n",
      "  mean: -0.017447900027036667\n",
      "  std: 0.973820686340332\n",
      "  min/max: -6.01171875 5.7109375\n",
      "  y counts: {0: 2083, 1: 2917}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sanity_mm_fp32(X_mm, y_mm, name=\"split\", n=5000):\n",
    "    n = min(n, len(y_mm))\n",
    "    X = np.array(X_mm[:n], dtype=np.float32)   # <- clave\n",
    "    y = np.array(y_mm[:n], dtype=np.int64)\n",
    "\n",
    "    print(f\"\\n[{name}]\")\n",
    "    print(\"  finite:\", np.isfinite(X).all())\n",
    "    print(\"  mean:\", float(X.mean()))\n",
    "    print(\"  std:\",  float(X.std()))\n",
    "    print(\"  min/max:\", float(X.min()), float(X.max()))\n",
    "    print(\"  y counts:\", {int(v): int((y==v).sum()) for v in np.unique(y)})\n",
    "\n",
    "sanity_mm_fp32(X_train, y_train, \"train\")\n",
    "sanity_mm_fp32(X_val,   y_val,   \"val\")\n",
    "sanity_mm_fp32(X_test,  y_test,  \"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea7275a1-987b-4f03-9aa9-1ef1448e6c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any inf: False\n",
      "Any nan: False\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X_train[:5000], dtype=np.float32)\n",
    "print(\"Any inf:\", np.isinf(X).any())\n",
    "print(\"Any nan:\", np.isnan(X).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40211f38-faf1-42e2-81f5-81b37583f113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity AUC (LogReg): 0.9303626521189672\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# usa una muestra pequeña para que sea rápido\n",
    "ntr = min(20000, len(y_train))\n",
    "nva = min(8000,  len(y_val))\n",
    "\n",
    "Xtr = np.array(X_train[:ntr], dtype=np.float32)\n",
    "ytr = np.array(y_train[:ntr], dtype=np.int64)\n",
    "\n",
    "Xva = np.array(X_val[:nva], dtype=np.float32)\n",
    "yva = np.array(y_val[:nva], dtype=np.int64)\n",
    "\n",
    "clf = LogisticRegression(max_iter=200, n_jobs=-1)\n",
    "clf.fit(Xtr, ytr)\n",
    "\n",
    "pva = clf.predict_proba(Xva)[:, 1]\n",
    "auc = roc_auc_score(yva, pva)\n",
    "print(\"Sanity AUC (LogReg):\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1f3f255-cfb4-4b0f-b7fb-693c129a2407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: processed/manifest_timesformer_cls.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "manifest = {\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"model\": TIMESFORMER_CKPT,\n",
    "    \"T\": T,\n",
    "    \"img_size\": IMG_SIZE,\n",
    "    \"embedding_dim\": int(X_train.shape[1]),\n",
    "    \"files\": {\n",
    "        \"X_train\": \"processed/emb_timesformer_cls_train.mmap\",\n",
    "        \"y_train\": \"processed/y_train.mmap\",\n",
    "        \"X_val\":   \"processed/emb_timesformer_cls_val.mmap\",\n",
    "        \"y_val\":   \"processed/y_val.mmap\",\n",
    "        \"X_test\":  \"processed/emb_timesformer_cls_test.mmap\",\n",
    "        \"y_test\":  \"processed/y_test.mmap\",\n",
    "    }\n",
    "}\n",
    "\n",
    "Path(\"processed\").mkdir(exist_ok=True)\n",
    "with open(\"processed/manifest_timesformer_cls.json\", \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", \"processed/manifest_timesformer_cls.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tesis)",
   "language": "python",
   "name": "tesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
